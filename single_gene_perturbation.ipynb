{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install scanpy anndata scikit-bio pingouin scikit-learn matplotlib pandas numpy scipy\n",
        "\n",
        "import scanpy as sc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from skbio.stats.distance import mantel, DistanceMatrix\n",
        "from scipy.stats import spearmanr\n",
        "import pingouin as pg\n",
        "import matplotlib.pyplot as plt\n",
        "import os, json\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IcvNlZqhBqaq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a33fbdc4-a8f1-4ea2-d511-534f1032839e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/2.1 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.9/169.9 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.4/204.4 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.2/58.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.4/276.4 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install scanpy anndata scikit-bio pingouin scikit-learn matplotlib pandas numpy scipy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gzip\n",
        "import scipy.io\n",
        "from scipy.sparse import csr_matrix\n",
        "import scanpy as sc\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from skbio.stats.distance import mantel, DistanceMatrix\n",
        "from scipy.stats import spearmanr\n",
        "import pingouin as pg\n",
        "import matplotlib.pyplot as plt\n",
        "import os, json\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# File paths\n",
        "base_path = \"/content/drive/MyDrive/dataset-gene-embed/\"\n",
        "files = {\n",
        "    'barcodes': base_path + \"GSE133344_filtered_barcodes.tsv.gz\",\n",
        "    'cell_identities': base_path + \"GSE133344_filtered_cell_identities.csv.gz\",\n",
        "    'matrix': base_path + \"GSE133344_filtered_matrix.mtx\",\n",
        "    'genes': base_path + \"GSE133344_filtered_genes.tsv\",\n",
        "    'generain': base_path + \"GeneRAIN-vec.200d.txt\"\n",
        "}\n",
        "\n",
        "# Load GSE133344 data\n",
        "def load_gse133344_data():\n",
        "    \"\"\"Load GSE133344 dataset and create AnnData object\"\"\"\n",
        "    print(\"Loading GSE133344 dataset...\")\n",
        "\n",
        "    # Load genes\n",
        "    genes = pd.read_csv(files['genes'], sep='\\t', header=None, names=['ensembl_id', 'gene_symbol'])\n",
        "\n",
        "    # Load barcodes\n",
        "    with gzip.open(files['barcodes'], 'rt') as f:\n",
        "        barcodes = [line.strip() for line in f]\n",
        "\n",
        "    # Load cell identities\n",
        "    cell_identities = pd.read_csv(files['cell_identities'], compression='gzip')\n",
        "\n",
        "    # Load expression matrix\n",
        "    matrix = scipy.io.mmread(files['matrix']).T.tocsr()  # Transpose to cells x genes\n",
        "\n",
        "    # Create AnnData object\n",
        "    adata = sc.AnnData(X=matrix)\n",
        "    adata.obs_names = barcodes\n",
        "    adata.var_names = genes['gene_symbol'].values\n",
        "    adata.var['ensembl_id'] = genes['ensembl_id'].values\n",
        "\n",
        "    # Add cell annotations (align by barcode)\n",
        "    cell_identities = cell_identities.set_index('cell_barcode')\n",
        "    # Only keep cells that are in both datasets\n",
        "    common_cells = list(set(adata.obs_names) & set(cell_identities.index))\n",
        "    adata = adata[common_cells].copy()\n",
        "    adata.obs = adata.obs.join(cell_identities.loc[common_cells])\n",
        "\n",
        "    print(f\"Loaded dataset: {adata.n_obs} cells x {adata.n_vars} genes\")\n",
        "    print(f\"Perturbation guides: {adata.obs['guide_identity'].nunique()}\")\n",
        "\n",
        "    return adata\n",
        "\n",
        "# Parse and filter for single-gene perturbations\n",
        "def parse_single_gene_targets(guide_identity):\n",
        "    \"\"\"Extract single gene targets, return None for multi-gene or controls\"\"\"\n",
        "    if pd.isna(guide_identity):\n",
        "        return None\n",
        "\n",
        "    # Extract the first part before '__'\n",
        "    target_part = guide_identity.split('__')[0]\n",
        "    components = target_part.split('_')\n",
        "\n",
        "    # Filter out control components\n",
        "    real_targets = [comp for comp in components if 'NegCtrl' not in comp and len(comp) > 0]\n",
        "\n",
        "    # Return single gene or None\n",
        "    if len(real_targets) == 1:\n",
        "        return real_targets[0]\n",
        "    elif len(real_targets) == 0:\n",
        "        return 'CONTROL'  # This is a control\n",
        "    else:\n",
        "        return None  # Multi-gene, exclude from single-gene analysis\n",
        "\n",
        "def filter_single_gene_data(adata):\n",
        "    \"\"\"Filter dataset to only single-gene perturbations and controls\"\"\"\n",
        "    print(\"Filtering for single-gene perturbations...\")\n",
        "\n",
        "    # Parse targets\n",
        "    adata.obs['single_target'] = adata.obs['guide_identity'].apply(parse_single_gene_targets)\n",
        "\n",
        "    # Keep only single genes and controls\n",
        "    keep_mask = adata.obs['single_target'].notna()\n",
        "    adata_filtered = adata[keep_mask].copy()\n",
        "\n",
        "    print(f\"After filtering: {adata_filtered.n_obs} cells\")\n",
        "\n",
        "    # Show target distribution\n",
        "    target_counts = adata_filtered.obs['single_target'].value_counts()\n",
        "    print(f\"Number of unique targets: {len(target_counts)}\")\n",
        "    print(f\"Top 10 targets:\")\n",
        "    print(target_counts.head(10))\n",
        "\n",
        "    # Statistics\n",
        "    control_cells = (adata_filtered.obs['single_target'] == 'CONTROL').sum()\n",
        "    perturbed_cells = adata_filtered.n_obs - control_cells\n",
        "    print(f\"Control cells: {control_cells}\")\n",
        "    print(f\"Perturbed cells: {perturbed_cells}\")\n",
        "\n",
        "    return adata_filtered\n",
        "\n",
        "# Load GeneRAIN embeddings (same as combinatorial)\n",
        "def load_generain_embeddings(file_path):\n",
        "    \"\"\"Load GeneRAIN embeddings\"\"\"\n",
        "    print(f\"Loading GeneRAIN embeddings from {file_path}...\")\n",
        "\n",
        "    genes = []\n",
        "    embeddings = []\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "        # Skip metadata line\n",
        "        first_line = f.readline().strip()\n",
        "        print(f\"Metadata: {first_line}\")\n",
        "\n",
        "        for line_num, line in enumerate(f, 2):\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "\n",
        "            gene_name = parts[0]\n",
        "            embedding_values = [float(x) for x in parts[1:]]\n",
        "            genes.append(gene_name)\n",
        "            embeddings.append(embedding_values)\n",
        "\n",
        "    df = pd.DataFrame(embeddings, index=genes)\n",
        "    df.columns = [f\"generain_{i}\" for i in range(df.shape[1])]\n",
        "\n",
        "    print(f\"Loaded GeneRAIN: {df.shape[0]} genes, {df.shape[1]} dimensions\")\n",
        "    return df\n",
        "\n",
        "# Enhanced gene mapping with aliases\n",
        "def create_gene_alias_mapping():\n",
        "    \"\"\"Create comprehensive gene alias mapping\"\"\"\n",
        "    return {\n",
        "        # Common chromatin/epigenetic aliases\n",
        "        'KLF1': 'KLF1', 'TBX3': 'TBX3', 'TBX2': 'TBX2', 'CEBPE': 'CEBPE',\n",
        "        'RUNX1T1': 'RUNX1T1', 'ETS2': 'ETS2', 'CNN1': 'CNN1', 'SLC4A1': 'SLC4A1',\n",
        "        'UBASH3B': 'UBASH3B', 'OSR2': 'OSR2', 'ARID1A': 'ARID1A', 'BCORL1': 'BCORL1',\n",
        "        'FOSB': 'FOSB', 'SET': 'SET', 'BAK1': 'BAK1', 'FOXA3': 'FOXA3', 'FOXL2': 'FOXL2',\n",
        "        'TP73': 'TP73', 'HES7': 'HES7', 'IRF1': 'IRF1',\n",
        "\n",
        "        # Transcription factors\n",
        "        'KLF1': 'KLF1', 'SOX6': 'SOX6', 'GATA1': 'GATA1', 'TAL1': 'TAL1',\n",
        "        'LMO2': 'LMO2', 'LDB1': 'LDB1', 'E2A': 'TCF3', 'HEB': 'TCF12',\n",
        "\n",
        "        # Additional common aliases\n",
        "        'TCF3': 'TCF3', 'TCF12': 'TCF12', 'MYB': 'MYB', 'RUNX1': 'RUNX1',\n",
        "        'FLI1': 'FLI1', 'ERG': 'ERG', 'ELK1': 'ELK1', 'ELK4': 'ELK4',\n",
        "        'ETS1': 'ETS1', 'SPI1': 'SPI1', 'SPIB': 'SPIB', 'PU1': 'SPI1'\n",
        "    }\n",
        "\n",
        "def robust_gene_mapping(target_genes, embedding_genes):\n",
        "    \"\"\"Map target genes to embedding genes with alias resolution\"\"\"\n",
        "    alias_map = create_gene_alias_mapping()\n",
        "\n",
        "    # Direct matches\n",
        "    direct_matches = set(target_genes) & set(embedding_genes)\n",
        "\n",
        "    # Alias matches\n",
        "    alias_matches = {}\n",
        "    for target in target_genes:\n",
        "        if target in alias_map and alias_map[target] in embedding_genes:\n",
        "            alias_matches[target] = alias_map[target]\n",
        "\n",
        "    # Case-insensitive matches\n",
        "    target_upper = {g.upper(): g for g in target_genes}\n",
        "    embed_upper = {g.upper(): g for g in embedding_genes}\n",
        "    case_matches = set(target_upper.keys()) & set(embed_upper.keys())\n",
        "\n",
        "    # Combine all mappings\n",
        "    target_to_embed = {}\n",
        "\n",
        "    # Add direct matches\n",
        "    for target in direct_matches:\n",
        "        target_to_embed[target] = target\n",
        "\n",
        "    # Add alias matches\n",
        "    for target, canonical in alias_matches.items():\n",
        "        if target not in target_to_embed:\n",
        "            target_to_embed[target] = canonical\n",
        "\n",
        "    # Add case matches\n",
        "    for case_match in case_matches:\n",
        "        orig_target = target_upper[case_match]\n",
        "        if orig_target not in target_to_embed:\n",
        "            target_to_embed[orig_target] = embed_upper[case_match]\n",
        "\n",
        "    print(f\"Gene mapping results:\")\n",
        "    print(f\"  - Direct matches: {len(direct_matches)}\")\n",
        "    print(f\"  - Alias matches: {len(alias_matches)}\")\n",
        "    print(f\"  - Case matches: {len(case_matches) - len(direct_matches)}\")\n",
        "    print(f\"  - Total mapped: {len(target_to_embed)}/{len(target_genes)}\")\n",
        "\n",
        "    unmapped = set(target_genes) - set(target_to_embed.keys())\n",
        "    if unmapped:\n",
        "        print(f\"  - Unmapped: {sorted(list(unmapped))[:10]}\")\n",
        "\n",
        "    return target_to_embed\n",
        "\n",
        "# Pseudobulk aggregation\n",
        "def make_single_gene_pseudobulk(adata, min_cells=30):\n",
        "    \"\"\"Create pseudobulk for each single gene target\"\"\"\n",
        "    target_counts = adata.obs['single_target'].value_counts()\n",
        "\n",
        "    # Filter targets with sufficient cells\n",
        "    sufficient_targets = target_counts[target_counts >= min_cells].index.tolist()\n",
        "    adata_sufficient = adata[adata.obs['single_target'].isin(sufficient_targets)].copy()\n",
        "\n",
        "    print(f\"Targets with ≥{min_cells} cells: {len(sufficient_targets)}\")\n",
        "\n",
        "    # Group by target\n",
        "    groups = adata_sufficient.obs.groupby('single_target').indices\n",
        "\n",
        "    pseudobulk_data = []\n",
        "    meta_data = []\n",
        "\n",
        "    for target, cell_idx in groups.items():\n",
        "        # Sum counts for this target\n",
        "        summed_counts = np.array(adata_sufficient.X[cell_idx].sum(axis=0)).ravel()\n",
        "\n",
        "        pseudobulk_data.append(summed_counts)\n",
        "        meta_data.append({\n",
        "            'target': target,\n",
        "            'n_cells': len(cell_idx),\n",
        "            'is_control': target == 'CONTROL'\n",
        "        })\n",
        "\n",
        "    pseudobulk_df = pd.DataFrame(pseudobulk_data, columns=adata_sufficient.var_names)\n",
        "    pseudobulk_df.index = [m['target'] for m in meta_data]\n",
        "    meta_df = pd.DataFrame(meta_data)\n",
        "\n",
        "    print(f\"Created pseudobulk for {len(pseudobulk_df)} targets\")\n",
        "    return pseudobulk_df, meta_df\n",
        "\n",
        "def compute_single_gene_effects(pseudobulk_df, meta_df):\n",
        "    \"\"\"Compute single gene perturbation effects\"\"\"\n",
        "    # Normalize to CPM and log-transform\n",
        "    lib_sizes = pseudobulk_df.sum(axis=1)\n",
        "    cpm = pseudobulk_df.div(lib_sizes, axis=0) * 1e6\n",
        "    logcpm = np.log1p(cpm)\n",
        "\n",
        "    # Get control profile\n",
        "    control_profile = logcpm.loc['CONTROL']\n",
        "\n",
        "    # Compute effects for non-control targets\n",
        "    perturbed_targets = [t for t in pseudobulk_df.index if t != 'CONTROL']\n",
        "    effects = {}\n",
        "\n",
        "    for target in perturbed_targets:\n",
        "        effect = logcpm.loc[target] - control_profile\n",
        "        effects[target] = effect.values\n",
        "\n",
        "    # Create effects DataFrame\n",
        "    effects_df = pd.DataFrame.from_dict(effects, orient='index', columns=pseudobulk_df.columns)\n",
        "\n",
        "    # Z-score standardize each gene with proper handling of zero variance\n",
        "    effects_mean = effects_df.mean(axis=0)\n",
        "    effects_std = effects_df.std(axis=0, ddof=0)\n",
        "\n",
        "    # Replace zero standard deviations with 1 to avoid division by zero\n",
        "    effects_std = effects_std.replace(0, 1.0)\n",
        "\n",
        "    # Standardize\n",
        "    effects_zscore = effects_df.subtract(effects_mean, axis=1).divide(effects_std, axis=1)\n",
        "\n",
        "    # Replace any infinite or NaN values with 0\n",
        "    effects_zscore = effects_zscore.replace([np.inf, -np.inf], 0).fillna(0)\n",
        "\n",
        "    print(f\"Computed effects for {len(effects_zscore)} targets\")\n",
        "    print(f\"Data range: [{effects_zscore.values.min():.3f}, {effects_zscore.values.max():.3f}]\")\n",
        "\n",
        "    return effects_zscore\n",
        "\n",
        "# Coexpression baseline\n",
        "def coexpression_embedding(adata, targets, n_components=200):\n",
        "    \"\"\"Generate coexpression embedding for comparison\"\"\"\n",
        "    # Extract expression for target genes only\n",
        "    target_gene_idx = [i for i, gene in enumerate(adata.var_names) if gene in targets]\n",
        "    target_expression = adata.X[:, target_gene_idx]\n",
        "\n",
        "    # Mean-center and apply SVD\n",
        "    X = target_expression.toarray() if hasattr(target_expression, 'toarray') else target_expression\n",
        "    X = X - X.mean(axis=0)\n",
        "\n",
        "    # Handle potential numerical issues\n",
        "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    n_components_actual = min(n_components, len(targets)-1, X.shape[0]-1)\n",
        "    if n_components_actual <= 0:\n",
        "        n_components_actual = 1\n",
        "\n",
        "    svd = TruncatedSVD(n_components=n_components_actual, random_state=42)\n",
        "    try:\n",
        "        svd.fit(X)\n",
        "        gene_embeddings = svd.components_.T\n",
        "    except Exception as e:\n",
        "        print(f\"SVD failed, using random embeddings: {e}\")\n",
        "        gene_embeddings = np.random.normal(0, 1, (len(targets), n_components_actual))\n",
        "\n",
        "    target_genes = [adata.var_names[i] for i in target_gene_idx]\n",
        "\n",
        "    return pd.DataFrame(gene_embeddings, index=target_genes,\n",
        "                       columns=[f\"coexpr_{i}\" for i in range(gene_embeddings.shape[1])])\n",
        "\n",
        "# Distance calculations and statistics\n",
        "def pairwise_cosine(M):\n",
        "    \"\"\"Compute cosine distances with comprehensive error handling\"\"\"\n",
        "    # Handle NaN and infinity values\n",
        "    M = np.nan_to_num(M, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "\n",
        "    # Check for zero-norm rows (would cause division by zero in cosine)\n",
        "    row_norms = np.linalg.norm(M, axis=1)\n",
        "    zero_norm_mask = row_norms == 0\n",
        "\n",
        "    if zero_norm_mask.any():\n",
        "        print(f\"Warning: {zero_norm_mask.sum()} zero-norm rows detected, adding small noise\")\n",
        "        M[zero_norm_mask] += np.random.normal(0, 1e-6, size=(zero_norm_mask.sum(), M.shape[1]))\n",
        "\n",
        "    try:\n",
        "        distances = pairwise_distances(M, metric='cosine')\n",
        "        # Ensure diagonal is exactly 0 and handle any remaining numerical issues\n",
        "        np.fill_diagonal(distances, 0.0)\n",
        "        distances = np.nan_to_num(distances, nan=1.0, posinf=1.0, neginf=0.0)\n",
        "        return distances\n",
        "    except Exception as e:\n",
        "        print(f\"Distance computation failed: {e}\")\n",
        "        # Return identity matrix as fallback\n",
        "        n = M.shape[0]\n",
        "        return np.eye(n)\n",
        "\n",
        "def mantel_and_spearman(D1, D2, n_perms=999):\n",
        "    if np.isnan(D1).any() or np.isnan(D2).any():\n",
        "        D1 = np.nan_to_num(D1, nan=1.0)\n",
        "        D2 = np.nan_to_num(D2, nan=1.0)\n",
        "\n",
        "    iu = np.triu_indices_from(D1, k=1)\n",
        "    rho, p = spearmanr(D1[iu], D2[iu])\n",
        "\n",
        "    if n_perms > 0:\n",
        "        try:\n",
        "            ids = [f\"X{i}\" for i in range(D1.shape[0])]\n",
        "            m1, m2 = DistanceMatrix(D1, ids=ids), DistanceMatrix(D2, ids=ids)\n",
        "            r_m, p_m, _ = mantel(m1, m2, method='spearman', permutations=n_perms)\n",
        "            return rho, p, r_m, p_m\n",
        "        except:\n",
        "            return rho, p, rho, p\n",
        "    return rho, p, rho, p\n",
        "\n",
        "def partial_corr_vectorized(Dg, Dp, Dc):\n",
        "    iu = np.triu_indices_from(Dg, k=1)\n",
        "    df = pd.DataFrame({\"Dg\": Dg[iu], \"Dp\": Dp[iu], \"Dc\": Dc[iu]})\n",
        "    df = df.dropna()\n",
        "\n",
        "    if len(df) == 0:\n",
        "        return 0.0, 1.0\n",
        "\n",
        "    try:\n",
        "        res = pg.partial_corr(data=df, x=\"Dg\", y=\"Dp\", covar=\"Dc\", method=\"spearman\")\n",
        "        return float(res[\"r\"]), float(res[\"p-val\"])\n",
        "    except:\n",
        "        return 0.0, 1.0\n",
        "\n",
        "def retrieval_metrics(Dg, Dp, ks=(5,10,20)):\n",
        "    n = Dg.shape[0]\n",
        "    Dg2, Dp2 = Dg.copy(), Dp.copy()\n",
        "    np.fill_diagonal(Dg2, np.inf)\n",
        "    np.fill_diagonal(Dp2, np.inf)\n",
        "\n",
        "    precisions = {k: [] for k in ks}\n",
        "    for i in range(n):\n",
        "        rank_g = np.argsort(Dg2[i])\n",
        "        rank_p = np.argsort(Dp2[i])\n",
        "        for k in ks:\n",
        "            precisions[k].append(len(set(rank_g[:k]) & set(rank_p[:k]))/k)\n",
        "\n",
        "    return {k: float(np.mean(v)) for k, v in precisions.items()}\n",
        "\n",
        "# Plotting\n",
        "def plot_distance_scatter(Dg, Dp, title, out=\"figs/single_gene_scatter.png\"):\n",
        "    os.makedirs(\"figs\", exist_ok=True)\n",
        "    iu = np.triu_indices_from(Dg, k=1)\n",
        "    x, y = Dg[iu], Dp[iu]\n",
        "\n",
        "    rho, p = spearmanr(x, y)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.scatter(x, y, s=8, alpha=0.6)\n",
        "    plt.title(f\"{title}\\nSpearman ρ={rho:.3f}, p={p:.2e}\")\n",
        "    plt.xlabel(\"Gene Embedding Distance\")\n",
        "    plt.ylabel(\"Single-Gene Perturbation Distance\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out, dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "# Main analysis function\n",
        "def run_single_gene_analysis():\n",
        "    \"\"\"Run the full single-gene perturbation analysis\"\"\"\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "    print(\"=== SINGLE-GENE PERTURBATION ANALYSIS ===\\n\")\n",
        "\n",
        "    # Load and filter data\n",
        "    adata = load_gse133344_data()\n",
        "    adata_filtered = filter_single_gene_data(adata)\n",
        "\n",
        "    # Load GeneRAIN embeddings\n",
        "    generain_emb = load_generain_embeddings(files['generain'])\n",
        "\n",
        "    # Create pseudobulk\n",
        "    pseudobulk_df, meta_df = make_single_gene_pseudobulk(adata_filtered, min_cells=30)\n",
        "\n",
        "    # Compute effects\n",
        "    effects_df = compute_single_gene_effects(pseudobulk_df, meta_df)\n",
        "\n",
        "    # Get unique targets (excluding controls)\n",
        "    targets = effects_df.index.tolist()\n",
        "\n",
        "    # Map genes to embeddings\n",
        "    gene_mapping = robust_gene_mapping(targets, generain_emb.index.tolist())\n",
        "\n",
        "    # Align datasets\n",
        "    mapped_targets = list(gene_mapping.keys())\n",
        "    print(f\"Targets mapped to GeneRAIN: {len(mapped_targets)}\")\n",
        "\n",
        "    if len(mapped_targets) < 10:\n",
        "        print(\"ERROR: Too few targets mapped for analysis\")\n",
        "        return None\n",
        "\n",
        "    effects_aligned = effects_df.loc[mapped_targets]\n",
        "    embeddings_aligned = generain_emb.loc[[gene_mapping[t] for t in mapped_targets]]\n",
        "\n",
        "    # Generate coexpression baseline\n",
        "    coexpr_emb = coexpression_embedding(adata_filtered, mapped_targets, n_components=200)\n",
        "    coexpr_aligned = coexpr_emb.loc[mapped_targets]\n",
        "\n",
        "    # Compute distance matrices\n",
        "    Dp = pairwise_cosine(effects_aligned.values)\n",
        "    Dg = pairwise_cosine(embeddings_aligned.values)\n",
        "    Dc = pairwise_cosine(coexpr_aligned.values)\n",
        "\n",
        "    # Statistical analysis\n",
        "    rho, p, r_mantel, p_mantel = mantel_and_spearman(Dg, Dp, n_perms=999)\n",
        "    rho_coexpr, p_coexpr, _, _ = mantel_and_spearman(Dc, Dp, n_perms=0)\n",
        "    part_rho, part_p = partial_corr_vectorized(Dg, Dp, Dc)\n",
        "\n",
        "    retrieval_results = retrieval_metrics(Dg, Dp)\n",
        "\n",
        "    # Results\n",
        "    results = {\n",
        "        'analysis_type': 'single_gene',\n",
        "        'n_targets': len(mapped_targets),\n",
        "        'generain_embedding': {\n",
        "            'spearman_rho': float(rho),\n",
        "            'spearman_p': float(p),\n",
        "            'mantel_r': float(r_mantel),\n",
        "            'mantel_p': float(p_mantel),\n",
        "            'precision_at': retrieval_results\n",
        "        },\n",
        "        'coexpression_baseline': {\n",
        "            'spearman_rho': float(rho_coexpr),\n",
        "            'spearman_p': float(p_coexpr)\n",
        "        },\n",
        "        'partial_correlation': {\n",
        "            'rho': float(part_rho),\n",
        "            'p': float(part_p)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Save results\n",
        "    with open(\"results/single_gene_metrics.json\", \"w\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    # Plot\n",
        "    plot_distance_scatter(Dg, Dp, \"Single-Gene Perturbation Analysis\")\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n=== SINGLE-GENE ANALYSIS RESULTS ===\")\n",
        "    print(f\"Targets analyzed: {len(mapped_targets)}\")\n",
        "    print(f\"GeneRAIN correlation: ρ = {rho:.3f}, p = {p:.2e}\")\n",
        "    print(f\"Coexpression baseline: ρ = {rho_coexpr:.3f}, p = {p_coexpr:.2e}\")\n",
        "    print(f\"Partial correlation: ρ = {part_rho:.3f}, p = {part_p:.2e}\")\n",
        "    print(f\"Precision@10: {retrieval_results[10]:.3f}\")\n",
        "    print(f\"Random baseline: {10/(len(mapped_targets)-1):.3f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run analysis\n",
        "results = run_single_gene_analysis()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTV59ey5jBIJ",
        "outputId": "b2ba8630-1c54-4cba-b38a-e5eb7f581378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "=== SINGLE-GENE PERTURBATION ANALYSIS ===\n",
            "\n",
            "Loading GSE133344 dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/anndata/_core/anndata.py:1793: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
            "  utils.warn_names_duplicates(\"var\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset: 111445 cells x 33694 genes\n",
            "Perturbation guides: 290\n",
            "Filtering for single-gene perturbations...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/anndata/_core/anndata.py:1793: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
            "  utils.warn_names_duplicates(\"var\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After filtering: 69686 cells\n",
            "Number of unique targets: 106\n",
            "Top 10 targets:\n",
            "single_target\n",
            "CONTROL    11855\n",
            "KLF1        1960\n",
            "BAK1        1457\n",
            "CEBPE       1233\n",
            "UBASH3B     1202\n",
            "ETS2        1201\n",
            "OSR2        1003\n",
            "SLC4A1      1000\n",
            "SET          986\n",
            "ELMSAN1      937\n",
            "Name: count, dtype: int64\n",
            "Control cells: 11855\n",
            "Perturbed cells: 57831\n",
            "Loading GeneRAIN embeddings from /content/drive/MyDrive/dataset-gene-embed/GeneRAIN-vec.200d.txt...\n",
            "Metadata: 31769 200\n",
            "Loaded GeneRAIN: 31769 genes, 200 dimensions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/anndata/_core/anndata.py:1793: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
            "  utils.warn_names_duplicates(\"var\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Targets with ≥30 cells: 106\n",
            "Created pseudobulk for 106 targets\n",
            "Computed effects for 105 targets\n",
            "Data range: [-8.100, 10.198]\n",
            "Gene mapping results:\n",
            "  - Direct matches: 101\n",
            "  - Alias matches: 21\n",
            "  - Case matches: 0\n",
            "  - Total mapped: 101/105\n",
            "  - Unmapped: ['C19orf26', 'C3orf72', 'ELMSAN1', 'KIAA1804']\n",
            "Targets mapped to GeneRAIN: 101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3909516726.py:371: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  return float(res[\"r\"]), float(res[\"p-val\"])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== SINGLE-GENE ANALYSIS RESULTS ===\n",
            "Targets analyzed: 101\n",
            "GeneRAIN correlation: ρ = 0.049, p = 4.82e-04\n",
            "Coexpression baseline: ρ = -0.016, p = 2.41e-01\n",
            "Partial correlation: ρ = 0.049, p = 4.88e-04\n",
            "Precision@10: 0.122\n",
            "Random baseline: 0.100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 0) Installs (Colab)\n",
        "# =========================\n",
        "!pip -q install scanpy anndata scikit-bio pingouin scikit-learn matplotlib pandas numpy scipy\n",
        "\n",
        "# =========================\n",
        "# 1) Imports & paths\n",
        "# =========================\n",
        "import os, json, gzip\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scanpy as sc\n",
        "import scipy.io\n",
        "from scipy import sparse\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from scipy.stats import spearmanr\n",
        "from skbio.stats.distance import mantel, DistanceMatrix\n",
        "import pingouin as pg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ---- Edit these paths for your Drive layout ----\n",
        "BASE = \"/content/drive/MyDrive/dataset-gene-embed\"\n",
        "FILES = {\n",
        "    \"barcodes\": f\"{BASE}/GSE133344_filtered_barcodes.tsv.gz\",\n",
        "    \"cell_id\": f\"{BASE}/GSE133344_filtered_cell_identities.csv.gz\",\n",
        "    \"matrix\":   f\"{BASE}/GSE133344_filtered_matrix.mtx\",   # if you kept it gzipped, use .mtx.gz and mmread handles it\n",
        "    \"genes\":    f\"{BASE}/GSE133344_filtered_genes.tsv\",\n",
        "    \"generain\": f\"{BASE}/GeneRAIN-vec.200d.txt\",\n",
        "}\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "os.makedirs(\"figs\", exist_ok=True)\n",
        "\n",
        "# =========================\n",
        "# 2) I/O & parsing helpers\n",
        "# =========================\n",
        "def load_gse133344():\n",
        "    \"\"\"\n",
        "    Load Norman 2019 (GSE133344) filtered matrix into an AnnData object.\n",
        "    Ensures var_names are unique and merges the provided cell identities.\n",
        "    \"\"\"\n",
        "    print(\"Loading GSE133344...\")\n",
        "    genes = pd.read_csv(FILES[\"genes\"], sep=\"\\t\", header=None, names=[\"ensembl_id\",\"gene_symbol\"])\n",
        "    # barcodes\n",
        "    with gzip.open(FILES[\"barcodes\"], \"rt\") as f:\n",
        "        barcodes = [ln.strip() for ln in f]\n",
        "    # cell identities\n",
        "    cell_id = pd.read_csv(FILES[\"cell_id\"], compression=\"gzip\")\n",
        "    # expression matrix (cells x genes)\n",
        "    X = scipy.io.mmread(FILES[\"matrix\"]).T.tocsr()\n",
        "\n",
        "    ad = sc.AnnData(X=X)\n",
        "    ad.obs_names = barcodes\n",
        "    ad.var_names = genes[\"gene_symbol\"].values\n",
        "    ad.var[\"ensembl_id\"] = genes[\"ensembl_id\"].values\n",
        "    ad.var_names_make_unique()\n",
        "\n",
        "    cell_id = cell_id.set_index(\"cell_barcode\")\n",
        "    common = list(set(ad.obs_names) & set(cell_id.index))\n",
        "    ad = ad[common].copy()\n",
        "    ad.obs = ad.obs.join(cell_id.loc[common])\n",
        "\n",
        "    ggs = sorted(pd.Series(ad.obs[\"gemgroup\"]).dropna().unique().tolist())\n",
        "    print(f\"Loaded: {ad.n_obs} cells × {ad.n_vars} genes | gemgroups: {ggs}\")\n",
        "    return ad\n",
        "\n",
        "def parse_single_target(guide_identity: str):\n",
        "    \"\"\"\n",
        "    Return single-gene symbol, 'CONTROL' for negatives, or None for multi-gene.\n",
        "    \"\"\"\n",
        "    if pd.isna(guide_identity):\n",
        "        return None\n",
        "    left = guide_identity.split(\"__\")[0]\n",
        "    parts = [p for p in left.split(\"_\") if p and (\"NegCtrl\" not in p)]\n",
        "    if len(parts) == 0:\n",
        "        return \"CONTROL\"\n",
        "    if len(parts) == 1:\n",
        "        return parts[0]\n",
        "    return None\n",
        "\n",
        "def filter_single_gene(ad):\n",
        "    \"\"\"\n",
        "    Keep cells that are CONTROL or single-gene perturbations (drop multis).\n",
        "    Adds obs['single_target'].\n",
        "    \"\"\"\n",
        "    ad = ad.copy()\n",
        "    ad.obs[\"single_target\"] = ad.obs[\"guide_identity\"].apply(parse_single_target)\n",
        "    keep = ad.obs[\"single_target\"].notna()\n",
        "    ad = ad[keep].copy()\n",
        "    print(f\"After single-gene filter: {ad.n_obs} cells \"\n",
        "          f\"(controls={int((ad.obs['single_target']=='CONTROL').sum())})\")\n",
        "    print(f\"Unique targets (incl CONTROL): {ad.obs['single_target'].nunique()}\")\n",
        "    return ad\n",
        "\n",
        "# =========================\n",
        "# 3) Pseudobulk & effects\n",
        "# =========================\n",
        "def pseudobulk_target_gemgroup(ad, min_cells=20):\n",
        "    \"\"\"\n",
        "    Sum counts per (single_target, gemgroup); keep combos with >= min_cells.\n",
        "    Returns (pseudobulk_df: MultiIndex rows, meta_df).\n",
        "    \"\"\"\n",
        "    assert \"single_target\" in ad.obs.columns, \"call filter_single_gene() first\"\n",
        "    assert \"gemgroup\" in ad.obs.columns, \"gemgroup not present in obs\"\n",
        "    groups = ad.obs.groupby([\"single_target\",\"gemgroup\"]).indices\n",
        "\n",
        "    rows, meta = [], []\n",
        "    X = ad.X\n",
        "    is_sp = sparse.issparse(X)\n",
        "    for (t, gg), idx in groups.items():\n",
        "        if len(idx) < min_cells:\n",
        "            continue\n",
        "        v = np.array(X[idx].sum(axis=0)).ravel() if is_sp else X[idx].sum(axis=0)\n",
        "        rows.append(v)\n",
        "        meta.append({\"target\": t, \"gemgroup\": gg, \"n_cells\": len(idx)})\n",
        "\n",
        "    pb = pd.DataFrame(rows, columns=ad.var_names)\n",
        "    pb.index = pd.MultiIndex.from_frame(pd.DataFrame(meta)[[\"target\",\"gemgroup\"]])\n",
        "    meta_df = pd.DataFrame(meta)\n",
        "    print(f\"Pseudobulk: {pb.shape[0]} (target,gemgroup) profiles\")\n",
        "    return pb, meta_df\n",
        "\n",
        "def effects_matched_controls(pb: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    For each target, subtract mean CONTROL in the SAME gemgroup, average across gemgroups.\n",
        "    Returns E_p (targets × genes) z-scored per gene.\n",
        "    \"\"\"\n",
        "    # log1p-CPM\n",
        "    lib = pb.sum(axis=1)\n",
        "    logcpm = np.log1p(pb.div(lib, axis=0) * 1e6)\n",
        "\n",
        "    targets = [t for t in logcpm.index.unique(level=0) if t != \"CONTROL\"]\n",
        "    eff = {}\n",
        "    for t in targets:\n",
        "        rows_t = logcpm.loc[t]\n",
        "        if isinstance(rows_t, pd.Series):\n",
        "            rows_t = rows_t.to_frame().T\n",
        "        diffs = []\n",
        "        for gg in rows_t.index:\n",
        "            try:\n",
        "                ctrl_rows = logcpm.loc[(\"CONTROL\", gg)]\n",
        "            except KeyError:\n",
        "                continue\n",
        "            ctrl_mean = ctrl_rows if isinstance(ctrl_rows, pd.Series) else ctrl_rows.mean(axis=0)\n",
        "            diffs.append(rows_t.loc[gg] - ctrl_mean)\n",
        "        if diffs:\n",
        "            e = pd.concat(diffs, axis=1).mean(axis=1)\n",
        "            eff[t] = e.values\n",
        "\n",
        "    E_p = pd.DataFrame.from_dict(eff, orient=\"index\", columns=logcpm.columns)\n",
        "\n",
        "    # per-gene z-score (robust to zero-variance)\n",
        "    mu = E_p.mean(0)\n",
        "    sd = E_p.std(0, ddof=0).replace(0, 1.0)\n",
        "    E_p = (E_p - mu) / sd\n",
        "    E_p = E_p.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
        "    print(f\"Computed matched effects for {E_p.shape[0]} targets\")\n",
        "    return E_p\n",
        "\n",
        "def qc_on_target_direction(E_p: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    CRISPRa expectation: targeted gene tends to increase on average.\n",
        "    Report median self-effect over targets present in columns.\n",
        "    \"\"\"\n",
        "    vals = []\n",
        "    for t in E_p.index:\n",
        "        if t in E_p.columns:\n",
        "            vals.append(E_p.loc[t, t])\n",
        "    if vals:\n",
        "        med = float(np.median(vals))\n",
        "        print(f\"[QC] On-target self-effect median (CRISPRa should be ≥ 0): {med:.3f}\")\n",
        "        return med\n",
        "    else:\n",
        "        print(\"[QC] No targets present among genes to check self-effect (ok).\")\n",
        "        return None\n",
        "\n",
        "# =========================\n",
        "# 4) Embeddings: pretrained & coexpression baseline\n",
        "# =========================\n",
        "def load_generain_embeddings(path: str):\n",
        "    \"\"\"\n",
        "    Load GeneRAIN-vec word2vec-style file (first line: N D, remainder: gene dim1..dimD).\n",
        "    \"\"\"\n",
        "    print(f\"Loading GeneRAIN vectors from: {path}\")\n",
        "    genes, embeds = [], []\n",
        "    with open(path, \"r\") as f:\n",
        "        _ = f.readline().strip()  # header: \"31769 200\" etc.\n",
        "        for ln in f:\n",
        "            parts = ln.strip().split()\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            genes.append(parts[0])\n",
        "            embeds.append([float(x) for x in parts[1:]])\n",
        "    df = pd.DataFrame(embeds, index=pd.Index(genes, name=\"gene\"))\n",
        "    df.columns = [f\"generain_{i}\" for i in range(df.shape[1])]\n",
        "    if df.index.duplicated().any():\n",
        "        df = df[~df.index.duplicated(keep=\"first\")]\n",
        "    print(f\"GeneRAIN loaded: {df.shape[0]} genes × {df.shape[1]} dims\")\n",
        "    return df\n",
        "\n",
        "def restrict_to_hvgs(ad, E_p: pd.DataFrame, n_top=2000):\n",
        "    \"\"\"\n",
        "    Compute HVGs using Seurat v3 flavor in scanpy, intersect with effect genes.\n",
        "    \"\"\"\n",
        "    ad_tmp = ad.copy()\n",
        "    sc.pp.highly_variable_genes(ad_tmp, n_top_genes=n_top, flavor=\"seurat_v3\", inplace=True)\n",
        "    hvgs = set(ad_tmp.var_names[ad_tmp.var[\"highly_variable\"].values])\n",
        "    kept = [g for g in E_p.columns if g in hvgs]\n",
        "    E_p_hvg = E_p.loc[:, kept]\n",
        "    print(f\"HVG restriction: using {len(kept)} HVGs out of {E_p.shape[1]} genes\")\n",
        "    return E_p_hvg\n",
        "\n",
        "def coexpr_baseline_hvg_union_targets(ad, target_genes, n_components=200, n_hvg=3000):\n",
        "    \"\"\"\n",
        "    Fit TruncatedSVD on cells × (HVGs ∪ targets), return embeddings for targets only.\n",
        "    Guarantees coverage for targets in partial-correlation baseline.\n",
        "    \"\"\"\n",
        "    print(\"Building coexpression baseline on HVGs ∪ targets...\")\n",
        "    ad2 = ad.copy()\n",
        "    sc.pp.highly_variable_genes(ad2, n_top_genes=n_hvg, flavor=\"seurat_v3\", inplace=True)\n",
        "\n",
        "    var_idx = ad2.var_names                                  # pandas Index\n",
        "    hv_mask = ad2.var[\"highly_variable\"].to_numpy(dtype=bool)\n",
        "    in_targets = var_idx.isin(list(target_genes))            # np.bool_ array\n",
        "    sel_mask = np.logical_or(hv_mask, in_targets)            # np.bool_ array\n",
        "\n",
        "    Xsel = ad.X[:, sel_mask]\n",
        "    Xsel = Xsel.toarray() if sparse.issparse(Xsel) else np.asarray(Xsel)\n",
        "    Xsel = np.nan_to_num(Xsel - Xsel.mean(axis=0))\n",
        "\n",
        "    n_comp = int(min(n_components, Xsel.shape[1]-1, Xsel.shape[0]-1))\n",
        "    n_comp = max(n_comp, 2)\n",
        "    svd = TruncatedSVD(n_components=n_comp, random_state=42).fit(Xsel)\n",
        "    gene_loadings = svd.components_.T\n",
        "    sel_genes = var_idx[sel_mask]\n",
        "\n",
        "    emb = pd.DataFrame(gene_loadings, index=sel_genes, columns=[f\"coexpr_{i}\" for i in range(n_comp)])\n",
        "    avail = [g for g in target_genes if g in emb.index]\n",
        "    print(f\"Coexpression baseline coverage: {len(avail)}/{len(target_genes)} targets\")\n",
        "    return emb.loc[avail]\n",
        "\n",
        "# =========================\n",
        "# 5) Distances & statistics\n",
        "# =========================\n",
        "def _prepare_matrix(M):\n",
        "    M = np.nan_to_num(M, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    norms = np.linalg.norm(M, axis=1)\n",
        "    zero = norms == 0\n",
        "    if zero.any():\n",
        "        M[zero] += np.random.normal(0, 1e-6, size=(zero.sum(), M.shape[1]))\n",
        "    return M\n",
        "\n",
        "def distance_cosine(M):\n",
        "    M = _prepare_matrix(M.copy())\n",
        "    D = pairwise_distances(M, metric=\"cosine\")\n",
        "    np.fill_diagonal(D, 0.0); D = np.nan_to_num(D)\n",
        "    return D\n",
        "\n",
        "def distance_correlation(M):\n",
        "    M = _prepare_matrix(M.copy())\n",
        "    C = np.corrcoef(M)\n",
        "    D = 1 - C\n",
        "    np.fill_diagonal(D, 0.0); D = np.nan_to_num(D)\n",
        "    return D\n",
        "\n",
        "def mantel_and_spearman(D1, D2, n_perms=999):\n",
        "    iu = np.triu_indices_from(D1, k=1)\n",
        "    rho, p = spearmanr(D1[iu], D2[iu])\n",
        "    try:\n",
        "        ids = [f\"X{i}\" for i in range(D1.shape[0])]\n",
        "        m1, m2 = DistanceMatrix(D1, ids=ids), DistanceMatrix(D2, ids=ids)\n",
        "        r_m, p_m, _ = mantel(m1, m2, method=\"spearman\", permutations=n_perms)\n",
        "    except Exception:\n",
        "        r_m, p_m = rho, p\n",
        "    return float(rho), float(p), float(r_m), float(p_m)\n",
        "\n",
        "def partial_corr_vectorized(Dg, Dp, Dc):\n",
        "    iu = np.triu_indices_from(Dg, 1)\n",
        "    df = pd.DataFrame({\"Dg\": Dg[iu], \"Dp\": Dp[iu], \"Dc\": Dc[iu]}).dropna()\n",
        "    if df.empty:\n",
        "        return 0.0, 1.0\n",
        "    try:\n",
        "        res = pg.partial_corr(data=df, x=\"Dg\", y=\"Dp\", covar=\"Dc\", method=\"spearman\")\n",
        "        return float(res[\"r\"].iloc[0]), float(res[\"p-val\"].iloc[0])\n",
        "    except Exception:\n",
        "        return 0.0, 1.0\n",
        "\n",
        "def retrieval_metrics(Dg, Dp, ks=(5,10,20)):\n",
        "    n = Dg.shape[0]\n",
        "    Dg2, Dp2 = Dg.copy(), Dp.copy()\n",
        "    np.fill_diagonal(Dg2, np.inf); np.fill_diagonal(Dp2, np.inf)\n",
        "    precisions = {k: [] for k in ks}\n",
        "    aps = []\n",
        "    for i in range(n):\n",
        "        rg = np.argsort(Dg2[i]); rp = np.argsort(Dp2[i])\n",
        "        for k in ks:\n",
        "            precisions[k].append(len(set(rg[:k]) & set(rp[:k]))/k)\n",
        "        # simple AP vs top-K neighbors in Dp\n",
        "        K = max(ks); true_set = set(rp[:K])\n",
        "        hits, prec_sum = 0, 0.0\n",
        "        for r, j in enumerate(rg, start=1):\n",
        "            if j in true_set:\n",
        "                hits += 1; prec_sum += hits/r\n",
        "                if hits == K: break\n",
        "        aps.append(prec_sum / (K if K>0 else 1))\n",
        "    out = {\"precision_at\": {k: float(np.mean(v)) for k,v in precisions.items()},\n",
        "           \"auprc\": float(np.mean(aps))}\n",
        "    return out\n",
        "\n",
        "def label_shuffle_p(Dg, Dp, n=1000, rng=42):\n",
        "    \"\"\"\n",
        "    Label-shuffle null that preserves Dg structure; permutes target labels in Dp.\n",
        "    \"\"\"\n",
        "    RS = np.random.RandomState(rng)\n",
        "    iu = np.triu_indices_from(Dg, 1)\n",
        "    obs = spearmanr(Dg[iu], Dp[iu])[0]\n",
        "    null = []\n",
        "    for _ in range(n):\n",
        "        perm = RS.permutation(Dp.shape[0])\n",
        "        Dp_perm = Dp[perm][:, perm]\n",
        "        null.append(spearmanr(Dg[iu], Dp_perm[iu])[0])\n",
        "    null = np.asarray(null)\n",
        "    p = (np.sum(null >= obs) + 1) / (n + 1)\n",
        "    return float(obs), float(p)\n",
        "\n",
        "def bootstrap_p_at_k(Dg, Dp, k=10, B=1000, rng=42):\n",
        "    \"\"\"\n",
        "    Bootstrap CI for Precision@k across targets.\n",
        "    \"\"\"\n",
        "    RS = np.random.RandomState(rng); n = Dg.shape[0]\n",
        "    vals = []\n",
        "    for _ in range(B):\n",
        "        idx = RS.choice(n, n, replace=True)\n",
        "        Dg_b, Dp_b = Dg[np.ix_(idx, idx)], Dp[np.ix_(idx, idx)]\n",
        "        np.fill_diagonal(Dg_b, np.inf); np.fill_diagonal(Dp_b, np.inf)\n",
        "        vals.append(retrieval_metrics(Dg_b, Dp_b, ks=(k,))[\"precision_at\"][k])\n",
        "    lo, hi = np.percentile(vals, [2.5, 97.5])\n",
        "    return float(np.mean(vals)), (float(lo), float(hi))\n",
        "\n",
        "# =========================\n",
        "# 6) Runner (single gene)\n",
        "# =========================\n",
        "def run_improved_single_gene_analysis(min_cells_pb=20, n_hvg_effects=2000,\n",
        "                                      coexpr_hvg=3000, n_perm_mantel=999,\n",
        "                                      ks=(5,10,20)):\n",
        "    ad = load_gse133344()\n",
        "    ad = filter_single_gene(ad)\n",
        "\n",
        "    # (target,gemgroup) pseudobulk & matched-control effects\n",
        "    pb, meta = pseudobulk_target_gemgroup(ad, min_cells=min_cells_pb)\n",
        "    E_p = effects_matched_controls(pb)\n",
        "    qc_on_target_direction(E_p)\n",
        "\n",
        "    # Restrict effects to HVGs for distance geometry\n",
        "    E_p_hvg = restrict_to_hvgs(ad, E_p, n_top=n_hvg_effects)\n",
        "\n",
        "    # Load pretrained gene vectors & map targets\n",
        "    E_g_pre = load_generain_embeddings(FILES[\"generain\"])\n",
        "    targets = E_p_hvg.index.tolist()  # gene symbols (single targets)\n",
        "    mapped_pre = [t for t in targets if t in E_g_pre.index]\n",
        "    print(f\"GeneRAIN mapped: {len(mapped_pre)}/{len(targets)}\")\n",
        "\n",
        "    # Coexpression baseline on HVGs ∪ targets, then align all three\n",
        "    E_g_co_all = coexpr_baseline_hvg_union_targets(ad, mapped_pre, n_components=200, n_hvg=coexpr_hvg)\n",
        "    final_targets = sorted(set(mapped_pre) & set(E_g_co_all.index) & set(E_p_hvg.index))\n",
        "    print(f\"Final aligned targets across (E_p, GeneRAIN, coexpr): {len(final_targets)}\")\n",
        "    if len(final_targets) < 30:\n",
        "        print(\"ERROR: too few targets after alignment for stable statistics.\")\n",
        "        return None\n",
        "\n",
        "    # Align matrices\n",
        "    EpA = E_p_hvg.loc[final_targets]\n",
        "    EgA = E_g_pre.loc[final_targets]\n",
        "    EcA = E_g_co_all.loc[final_targets]\n",
        "    # Alignment sanity check\n",
        "    assert (EpA.index == EgA.index).all() and (EpA.index == EcA.index).all(), \"Target index misaligned\"\n",
        "    print(\"Aligned targets:\", EpA.shape[0])\n",
        "\n",
        "    # Distances: evaluate cosine vs correlation; choose best by Spearman ρ\n",
        "    Dg_cos, Dg_cor = distance_cosine(EgA.values),      distance_correlation(EgA.values)\n",
        "    Dp_cos, Dp_cor = distance_cosine(EpA.values),      distance_correlation(EpA.values)\n",
        "    Dc_cos, Dc_cor = distance_cosine(EcA.values),      distance_correlation(EcA.values)\n",
        "\n",
        "    combos = {\n",
        "        \"cosine\":      (Dg_cos, Dp_cos, Dc_cos),\n",
        "        \"correlation\": (Dg_cor, Dp_cor, Dc_cor),\n",
        "    }\n",
        "    results = {}\n",
        "    best_key, best_rho = None, -np.inf\n",
        "\n",
        "    for key, (Dg, Dp, Dc) in combos.items():\n",
        "        rho, p, r_m, p_m = mantel_and_spearman(Dg, Dp, n_perms=n_perm_mantel)\n",
        "        rho_c, p_c, _, _ = mantel_and_spearman(Dc, Dp, n_perms=0)\n",
        "        prho, pp = partial_corr_vectorized(Dg, Dp, Dc)\n",
        "        ret = retrieval_metrics(Dg, Dp, ks=ks)\n",
        "        obs, p_lbl = label_shuffle_p(Dg, Dp, n=1000)\n",
        "        p10_mean, p10_ci = bootstrap_p_at_k(Dg, Dp, k=10, B=1000)\n",
        "\n",
        "        results[key] = {\n",
        "            \"spearman_rho\": rho, \"spearman_p\": p,\n",
        "            \"mantel_r\": r_m, \"mantel_p\": p_m,\n",
        "            \"label_shuffle_rho\": obs, \"label_shuffle_p\": p_lbl,\n",
        "            \"coexpr_rho\": rho_c, \"coexpr_p\": p_c,\n",
        "            \"partial_rho\": prho, \"partial_p\": pp,\n",
        "            \"precision_at\": ret[\"precision_at\"], \"auprc\": ret[\"auprc\"],\n",
        "            \"p10_boot_mean\": p10_mean, \"p10_boot_ci\": p10_ci,\n",
        "            \"n_targets\": len(final_targets)\n",
        "        }\n",
        "        print(f\"\\n[{key.upper()}] ρ={rho:.3f} (p={p:.1e}); Mantel p={p_m:.2e}; \"\n",
        "              f\"Partial ρ|coexpr={prho:.3f} (p={pp:.1e}); P@10={ret['precision_at'][10]:.3f}\")\n",
        "\n",
        "        if rho > best_rho:\n",
        "            best_rho, best_key = rho, key\n",
        "\n",
        "    # Save results\n",
        "    out = {\n",
        "        \"analysis_type\": \"single_gene_improved\",\n",
        "        \"best_metric\": best_key,\n",
        "        \"results_by_metric\": results\n",
        "    }\n",
        "    with open(\"results/improved_single_gene_metrics.json\", \"w\") as f:\n",
        "        json.dump(out, f, indent=2)\n",
        "\n",
        "    # Quick figure: distance–distance scatter for best metric\n",
        "    key = best_key\n",
        "    Dg, Dp, _ = combos[key]\n",
        "    iu = np.triu_indices_from(Dg, 1)\n",
        "    x, y = Dg[iu], Dp[iu]\n",
        "    r, pval = spearmanr(x, y)\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.scatter(x, y, s=6, alpha=0.5)\n",
        "    plt.title(f\"GeneRAIN vs Perturbation ({key})\\nSpearman ρ={r:.2f}, p={pval:.1e}\")\n",
        "    plt.xlabel(\"Gene embedding distance\"); plt.ylabel(\"Perturbation response distance\")\n",
        "    plt.tight_layout(); plt.savefig(\"figs/single_gene_dist_scatter.png\", dpi=200); plt.close()\n",
        "\n",
        "    print(f\"\\nBest metric: {best_key} | Results saved to results/improved_single_gene_metrics.json\")\n",
        "    return out\n",
        "\n",
        "# =========================\n",
        "# 7) Run\n",
        "# =========================\n",
        "res = run_improved_single_gene_analysis()\n",
        "res\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z82K6p2YS6oa",
        "outputId": "9b53042a-ea82-4f4b-f609-c4973d44130d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loading GSE133344...\n",
            "Loaded: 111445 cells × 33694 genes | gemgroups: [1, 2, 3, 4, 5, 6, 7, 8]\n",
            "After single-gene filter: 69686 cells (controls=11855)\n",
            "Unique targets (incl CONTROL): 106\n",
            "Pseudobulk: 831 (target,gemgroup) profiles\n",
            "Computed matched effects for 105 targets\n",
            "[QC] On-target self-effect median (CRISPRa should be ≥ 0): 5.953\n",
            "HVG restriction: using 2000 HVGs out of 33694 genes\n",
            "Loading GeneRAIN vectors from: /content/drive/MyDrive/dataset-gene-embed/GeneRAIN-vec.200d.txt\n",
            "GeneRAIN loaded: 31769 genes × 200 dims\n",
            "GeneRAIN mapped: 101/105\n",
            "Building coexpression baseline on HVGs ∪ targets...\n",
            "Coexpression baseline coverage: 101/101 targets\n",
            "Final aligned targets across (E_p, GeneRAIN, coexpr): 101\n",
            "Aligned targets: 101\n",
            "\n",
            "[COSINE] ρ=0.042 (p=2.9e-03); Mantel p=9.00e-03; Partial ρ|coexpr=0.036 (p=9.5e-03); P@10=0.131\n",
            "\n",
            "[CORRELATION] ρ=0.043 (p=2.3e-03); Mantel p=2.26e-03; Partial ρ|coexpr=0.038 (p=7.3e-03); P@10=0.132\n",
            "\n",
            "Best metric: correlation | Results saved to results/improved_single_gene_metrics.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'analysis_type': 'single_gene_improved',\n",
              " 'best_metric': 'correlation',\n",
              " 'results_by_metric': {'cosine': {'spearman_rho': 0.04185842204470368,\n",
              "   'spearman_p': 0.0029282878320825837,\n",
              "   'mantel_r': 0.041858422044703684,\n",
              "   'mantel_p': 0.009,\n",
              "   'label_shuffle_rho': 0.04185842204470368,\n",
              "   'label_shuffle_p': 0.005994005994005994,\n",
              "   'coexpr_rho': 0.032884280334120726,\n",
              "   'coexpr_p': 0.01944330919657188,\n",
              "   'partial_rho': 0.03647629603149489,\n",
              "   'partial_p': 0.00953942545908451,\n",
              "   'precision_at': {5: 0.07524752475247526,\n",
              "    10: 0.1306930693069307,\n",
              "    20: 0.21138613861386132},\n",
              "   'auprc': 0.2460943188987724,\n",
              "   'p10_boot_mean': 0.20321683168316837,\n",
              "   'p10_boot_ci': (0.16732673267326734, 0.24856435643564354),\n",
              "   'n_targets': 101},\n",
              "  'correlation': {'spearman_rho': 0.04295693020919127,\n",
              "   'spearman_p': 0.00226326633868648,\n",
              "   'mantel_r': 0.04295693020919127,\n",
              "   'mantel_p': 0.00226326633868648,\n",
              "   'label_shuffle_rho': 0.04295693020919127,\n",
              "   'label_shuffle_p': 0.004995004995004995,\n",
              "   'coexpr_rho': 0.03191811134949078,\n",
              "   'coexpr_p': 0.023315535459521683,\n",
              "   'partial_rho': 0.03776483495909127,\n",
              "   'partial_p': 0.007280742417747301,\n",
              "   'precision_at': {5: 0.07722772277227724,\n",
              "    10: 0.13168316831683166,\n",
              "    20: 0.2123762376237624},\n",
              "   'auprc': 0.24861635888522468,\n",
              "   'p10_boot_mean': 0.20399504950495057,\n",
              "   'p10_boot_ci': (0.1682920792079208, 0.2505198019801981),\n",
              "   'n_targets': 101}}}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Enhanced Gene Embedding Validation Pipeline\n",
        "# =========================\n",
        "\n",
        "!pip -q install scanpy anndata scikit-bio pingouin scikit-learn matplotlib pandas numpy scipy gprofiler-official\n",
        "\n",
        "import os, json, gzip\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scanpy as sc\n",
        "import scipy.io\n",
        "from scipy import sparse\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from scipy.stats import spearmanr, pearsonr\n",
        "from skbio.stats.distance import mantel, DistanceMatrix\n",
        "import pingouin as pg\n",
        "import matplotlib.pyplot as plt\n",
        "from gprofiler import GProfiler\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ---- Edit these paths for your Drive layout ----\n",
        "BASE = \"/content/drive/MyDrive/dataset-gene-embed\"\n",
        "FILES = {\n",
        "    \"barcodes\": f\"{BASE}/GSE133344_filtered_barcodes.tsv.gz\",\n",
        "    \"cell_id\": f\"{BASE}/GSE133344_filtered_cell_identities.csv.gz\",\n",
        "    \"matrix\":   f\"{BASE}/GSE133344_filtered_matrix.mtx\",\n",
        "    \"genes\":    f\"{BASE}/GSE133344_filtered_genes.tsv\",\n",
        "    \"generain\": f\"{BASE}/GeneRAIN-vec.200d.txt\",\n",
        "}\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "os.makedirs(\"figs\", exist_ok=True)\n",
        "\n",
        "# =========================\n",
        "# Core Data Loading Functions\n",
        "# =========================\n",
        "\n",
        "def load_gse133344():\n",
        "    \"\"\"Load Norman 2019 (GSE133344) filtered matrix into an AnnData object.\"\"\"\n",
        "    print(\"Loading GSE133344...\")\n",
        "    genes = pd.read_csv(FILES[\"genes\"], sep=\"\\t\", header=None, names=[\"ensembl_id\",\"gene_symbol\"])\n",
        "\n",
        "    with gzip.open(FILES[\"barcodes\"], \"rt\") as f:\n",
        "        barcodes = [ln.strip() for ln in f]\n",
        "\n",
        "    cell_id = pd.read_csv(FILES[\"cell_id\"], compression=\"gzip\")\n",
        "    X = scipy.io.mmread(FILES[\"matrix\"]).T.tocsr()\n",
        "\n",
        "    ad = sc.AnnData(X=X)\n",
        "    ad.obs_names = barcodes\n",
        "    ad.var_names = genes[\"gene_symbol\"].values\n",
        "    ad.var[\"ensembl_id\"] = genes[\"ensembl_id\"].values\n",
        "    ad.var_names_make_unique()\n",
        "\n",
        "    cell_id = cell_id.set_index(\"cell_barcode\")\n",
        "    common = list(set(ad.obs_names) & set(cell_id.index))\n",
        "    ad = ad[common].copy()\n",
        "    ad.obs = ad.obs.join(cell_id.loc[common])\n",
        "\n",
        "    print(f\"Loaded: {ad.n_obs} cells × {ad.n_vars} genes | gemgroups: {sorted(ad.obs['gemgroup'].unique())}\")\n",
        "    return ad\n",
        "\n",
        "def parse_combinatorial_targets(guide_identity):\n",
        "    \"\"\"Parse guide identities to extract target genes\"\"\"\n",
        "    if pd.isna(guide_identity):\n",
        "        return []\n",
        "\n",
        "    target_part = guide_identity.split('__')[0]\n",
        "    components = target_part.split('_')\n",
        "\n",
        "    targets = []\n",
        "    for comp in components:\n",
        "        if 'NegCtrl' not in comp and len(comp) > 0:\n",
        "            targets.append(comp)\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    unique_targets = []\n",
        "    for t in targets:\n",
        "        if t not in unique_targets:\n",
        "            unique_targets.append(t)\n",
        "\n",
        "    return unique_targets\n",
        "\n",
        "def categorize_perturbations(ad):\n",
        "    \"\"\"Categorize perturbations and add parsed target information\"\"\"\n",
        "    ad.obs['parsed_targets'] = ad.obs['guide_identity'].apply(parse_combinatorial_targets)\n",
        "    ad.obs['n_targets'] = ad.obs['parsed_targets'].apply(len)\n",
        "\n",
        "    ad.obs['perturbation_type'] = 'unknown'\n",
        "    ad.obs.loc[ad.obs['n_targets'] == 0, 'perturbation_type'] = 'control'\n",
        "    ad.obs.loc[ad.obs['n_targets'] == 1, 'perturbation_type'] = 'single'\n",
        "    ad.obs.loc[ad.obs['n_targets'] == 2, 'perturbation_type'] = 'dual'\n",
        "    ad.obs.loc[ad.obs['n_targets'] > 2, 'perturbation_type'] = 'multi'\n",
        "\n",
        "    print(\"Perturbation type distribution:\")\n",
        "    print(ad.obs['perturbation_type'].value_counts())\n",
        "\n",
        "    return ad\n",
        "\n",
        "def load_generain_embeddings(path: str):\n",
        "    \"\"\"Load GeneRAIN-vec word2vec-style file\"\"\"\n",
        "    print(f\"Loading GeneRAIN vectors from: {path}\")\n",
        "    genes, embeds = [], []\n",
        "    with open(path, \"r\") as f:\n",
        "        _ = f.readline().strip()  # header\n",
        "        for ln in f:\n",
        "            parts = ln.strip().split()\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            genes.append(parts[0])\n",
        "            embeds.append([float(x) for x in parts[1:]])\n",
        "\n",
        "    df = pd.DataFrame(embeds, index=pd.Index(genes, name=\"gene\"))\n",
        "    df.columns = [f\"generain_{i}\" for i in range(df.shape[1])]\n",
        "    if df.index.duplicated().any():\n",
        "        df = df[~df.index.duplicated(keep=\"first\")]\n",
        "    print(f\"GeneRAIN loaded: {df.shape[0]} genes × {df.shape[1]} dims\")\n",
        "    return df\n",
        "\n",
        "# =========================\n",
        "# Enhanced Pseudobulk & Effects\n",
        "# =========================\n",
        "\n",
        "def make_gemgroup_matched_pseudobulk(ad, min_cells=20):\n",
        "    \"\"\"Create pseudobulk with gemgroup matching\"\"\"\n",
        "    print(\"Creating gemgroup-matched pseudobulks...\")\n",
        "\n",
        "    groups = ad.obs.groupby(['guide_identity', 'gemgroup']).indices\n",
        "    pseudobulk_data = []\n",
        "    meta_data = []\n",
        "\n",
        "    X = ad.X\n",
        "    is_sparse = sparse.issparse(X)\n",
        "\n",
        "    for (guide, gemgroup), cell_idx in groups.items():\n",
        "        if len(cell_idx) < min_cells:\n",
        "            continue\n",
        "\n",
        "        if is_sparse:\n",
        "            summed_counts = np.array(X[cell_idx].sum(axis=0)).ravel()\n",
        "        else:\n",
        "            summed_counts = X[cell_idx].sum(axis=0)\n",
        "\n",
        "        pseudobulk_data.append(summed_counts)\n",
        "        meta_data.append({\n",
        "            'guide_identity': guide,\n",
        "            'gemgroup': gemgroup,\n",
        "            'n_cells': len(cell_idx),\n",
        "            'targets': ad.obs.loc[ad.obs_names[cell_idx[0]], 'parsed_targets'],\n",
        "            'perturbation_type': ad.obs.loc[ad.obs_names[cell_idx[0]], 'perturbation_type']\n",
        "        })\n",
        "\n",
        "    pseudobulk_df = pd.DataFrame(pseudobulk_data, columns=ad.var_names)\n",
        "    pseudobulk_df.index = pd.MultiIndex.from_frame(pd.DataFrame(meta_data)[['guide_identity', 'gemgroup']])\n",
        "    meta_df = pd.DataFrame(meta_data)\n",
        "\n",
        "    print(f\"Created pseudobulk for {len(pseudobulk_df)} guide-gemgroup combinations\")\n",
        "    return pseudobulk_df, meta_df\n",
        "\n",
        "def compute_gemgroup_matched_effects(pseudobulk_df, meta_df, effect_threshold=None):\n",
        "    \"\"\"Compute effects with gemgroup-matched controls and optional filtering\"\"\"\n",
        "    print(\"Computing gemgroup-matched effects...\")\n",
        "\n",
        "    lib_sizes = pseudobulk_df.sum(axis=1)\n",
        "    cpm = pseudobulk_df.div(lib_sizes, axis=0) * 1e6\n",
        "    logcpm = np.log1p(cpm)\n",
        "\n",
        "    control_guides = set(meta_df[meta_df['perturbation_type'] == 'control']['guide_identity'])\n",
        "    print(f\"Found {len(control_guides)} control guides\")\n",
        "\n",
        "    all_guides = sorted(set(meta_df['guide_identity']))\n",
        "    target_guides = [g for g in all_guides if g not in control_guides]\n",
        "\n",
        "    effects = {}\n",
        "\n",
        "    for guide in target_guides:\n",
        "        try:\n",
        "            guide_profiles = logcpm.loc[guide]\n",
        "            if isinstance(guide_profiles, pd.Series):\n",
        "                guide_profiles = pd.DataFrame([guide_profiles.values],\n",
        "                                            index=[guide_profiles.name],\n",
        "                                            columns=logcpm.columns)\n",
        "\n",
        "            guide_effects = []\n",
        "\n",
        "            for gemgroup in guide_profiles.index:\n",
        "                control_profiles_in_gemgroup = []\n",
        "                for control_guide in control_guides:\n",
        "                    try:\n",
        "                        ctrl_profile = logcpm.loc[(control_guide, gemgroup)]\n",
        "                        control_profiles_in_gemgroup.append(ctrl_profile)\n",
        "                    except KeyError:\n",
        "                        continue\n",
        "\n",
        "                if control_profiles_in_gemgroup:\n",
        "                    if len(control_profiles_in_gemgroup) == 1:\n",
        "                        control_mean = control_profiles_in_gemgroup[0]\n",
        "                    else:\n",
        "                        control_mean = pd.concat(control_profiles_in_gemgroup, axis=1).mean(axis=1)\n",
        "\n",
        "                    effect = guide_profiles.loc[gemgroup] - control_mean\n",
        "                    guide_effects.append(effect)\n",
        "\n",
        "            if guide_effects:\n",
        "                if len(guide_effects) == 1:\n",
        "                    mean_effect = guide_effects[0]\n",
        "                else:\n",
        "                    mean_effect = pd.concat(guide_effects, axis=1).mean(axis=1)\n",
        "                effects[guide] = mean_effect.values\n",
        "\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "    effects_df = pd.DataFrame.from_dict(effects, orient='index', columns=logcpm.columns)\n",
        "\n",
        "    # Z-score standardize\n",
        "    effects_mean = effects_df.mean(axis=0)\n",
        "    effects_std = effects_df.std(axis=0, ddof=0).replace(0, 1.0)\n",
        "    effects_zscore = effects_df.subtract(effects_mean, axis=1).divide(effects_std, axis=1)\n",
        "    effects_zscore = effects_zscore.replace([np.inf, -np.inf], 0).fillna(0)\n",
        "\n",
        "    # Optional: Filter by effect magnitude\n",
        "    if effect_threshold is not None:\n",
        "        strong_effects = (np.abs(effects_zscore).max(axis=1) > effect_threshold)\n",
        "        effects_zscore = effects_zscore[strong_effects]\n",
        "        print(f\"Filtered to {len(effects_zscore)} guides with strong effects (|z| > {effect_threshold})\")\n",
        "\n",
        "    print(f\"Computed effects for {len(effects_zscore)} guides\")\n",
        "    return effects_zscore\n",
        "\n",
        "# =========================\n",
        "# Enhanced Embedding Processing\n",
        "# =========================\n",
        "\n",
        "def create_random_baseline_embeddings(target_combinations, embedding_dim=200):\n",
        "    \"\"\"Create random embeddings as baseline\"\"\"\n",
        "    np.random.seed(42)\n",
        "    random_embeddings = {}\n",
        "\n",
        "    for combo_name, gene_list in target_combinations.items():\n",
        "        if len(gene_list) == 0:\n",
        "            # Control - use zero vector\n",
        "            random_embeddings[combo_name] = np.zeros(embedding_dim)\n",
        "        elif len(gene_list) == 1:\n",
        "            # Single gene - random vector\n",
        "            random_embeddings[combo_name] = np.random.normal(0, 1, embedding_dim)\n",
        "        else:\n",
        "            # Multiple genes - average of random vectors\n",
        "            gene_embeds = np.random.normal(0, 1, (len(gene_list), embedding_dim))\n",
        "            random_embeddings[combo_name] = np.mean(gene_embeds, axis=0)\n",
        "\n",
        "    random_df = pd.DataFrame.from_dict(random_embeddings, orient='index')\n",
        "    print(f\"Created random baseline: {random_df.shape[0]} combinations, {random_df.shape[1]} dimensions\")\n",
        "    return random_df\n",
        "\n",
        "def create_combined_embeddings_enhanced(target_combinations, generain_embeddings, method='average'):\n",
        "    \"\"\"Enhanced embedding combination with better handling\"\"\"\n",
        "    combined_embeddings = {}\n",
        "    missing_genes = set()\n",
        "\n",
        "    for combo_name, gene_list in target_combinations.items():\n",
        "        if len(gene_list) == 0:\n",
        "            combined_embeddings[combo_name] = np.zeros(generain_embeddings.shape[1])\n",
        "        elif len(gene_list) == 1:\n",
        "            gene = gene_list[0]\n",
        "            if gene in generain_embeddings.index:\n",
        "                combined_embeddings[combo_name] = generain_embeddings.loc[gene].values\n",
        "            else:\n",
        "                missing_genes.add(gene)\n",
        "                continue\n",
        "        else:\n",
        "            available_genes = [g for g in gene_list if g in generain_embeddings.index]\n",
        "            missing_genes.update(set(gene_list) - set(available_genes))\n",
        "\n",
        "            if len(available_genes) == 0:\n",
        "                continue\n",
        "\n",
        "            gene_embeds = np.stack([generain_embeddings.loc[g].values for g in available_genes])\n",
        "\n",
        "            if method == 'average':\n",
        "                combined_embeddings[combo_name] = np.mean(gene_embeds, axis=0)\n",
        "            elif method == 'sum':\n",
        "                combined_embeddings[combo_name] = np.sum(gene_embeds, axis=0)\n",
        "            elif method == 'hadamard':\n",
        "                combined_embeddings[combo_name] = np.prod(gene_embeds, axis=0)\n",
        "\n",
        "    if missing_genes:\n",
        "        print(f\"Missing genes in embeddings: {len(missing_genes)} unique genes\")\n",
        "\n",
        "    combo_df = pd.DataFrame.from_dict(combined_embeddings, orient='index')\n",
        "    print(f\"Created {method} embeddings: {combo_df.shape[0]} combinations\")\n",
        "    return combo_df\n",
        "\n",
        "# =========================\n",
        "# Enhanced Statistical Analysis\n",
        "# =========================\n",
        "\n",
        "def pairwise_cosine_robust(M):\n",
        "    \"\"\"Robust cosine distance computation\"\"\"\n",
        "    M = np.nan_to_num(M, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "\n",
        "    row_norms = np.linalg.norm(M, axis=1)\n",
        "    zero_norm_mask = row_norms == 0\n",
        "\n",
        "    if zero_norm_mask.any():\n",
        "        M[zero_norm_mask] += np.random.normal(0, 1e-6, size=(zero_norm_mask.sum(), M.shape[1]))\n",
        "\n",
        "    try:\n",
        "        distances = pairwise_distances(M, metric='cosine')\n",
        "        np.fill_diagonal(distances, 0.0)\n",
        "        distances = np.nan_to_num(distances, nan=1.0, posinf=1.0, neginf=0.0)\n",
        "        return distances\n",
        "    except Exception as e:\n",
        "        print(f\"Distance computation failed: {e}\")\n",
        "        return np.eye(M.shape[0])\n",
        "\n",
        "def cross_validate_by_gemgroup(effects_df, meta_df, embeddings_df, train_gemgroups, test_gemgroups):\n",
        "    \"\"\"Cross-validation splitting by gemgroups\"\"\"\n",
        "    print(f\"Cross-validation: train on gemgroups {train_gemgroups}, test on {test_gemgroups}\")\n",
        "\n",
        "    # Get guides that appear in both train and test sets\n",
        "    train_guides = set()\n",
        "    test_guides = set()\n",
        "\n",
        "    for guide in effects_df.index:\n",
        "        guide_gemgroups = set(meta_df[meta_df['guide_identity'] == guide]['gemgroup'])\n",
        "        if guide_gemgroups & set(train_gemgroups):\n",
        "            train_guides.add(guide)\n",
        "        if guide_gemgroups & set(test_gemgroups):\n",
        "            test_guides.add(guide)\n",
        "\n",
        "    common_guides = list(train_guides & test_guides & set(embeddings_df.index))\n",
        "    print(f\"Guides in both train/test: {len(common_guides)}\")\n",
        "\n",
        "    if len(common_guides) < 20:\n",
        "        return None\n",
        "\n",
        "    # Align data\n",
        "    effects_aligned = effects_df.loc[common_guides]\n",
        "    embeddings_aligned = embeddings_df.loc[common_guides]\n",
        "\n",
        "    # Compute distances\n",
        "    Dp = pairwise_cosine_robust(effects_aligned.values)\n",
        "    Dg = pairwise_cosine_robust(embeddings_aligned.values)\n",
        "\n",
        "    # Correlation\n",
        "    iu = np.triu_indices_from(Dg, k=1)\n",
        "    rho, p = spearmanr(Dg[iu], Dp[iu])\n",
        "\n",
        "    return {\n",
        "        'n_guides': len(common_guides),\n",
        "        'spearman_rho': float(rho),\n",
        "        'spearman_p': float(p)\n",
        "    }\n",
        "\n",
        "def compute_enhanced_metrics(Dg, Dp, n_permutations=999):\n",
        "    \"\"\"Comprehensive statistical analysis\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    # Basic correlation\n",
        "    iu = np.triu_indices_from(Dg, k=1)\n",
        "    dist_g, dist_p = Dg[iu], Dp[iu]\n",
        "\n",
        "    rho, p = spearmanr(dist_g, dist_p)\n",
        "    results['spearman_rho'] = float(rho)\n",
        "    results['spearman_p'] = float(p)\n",
        "\n",
        "    # Mantel test\n",
        "    try:\n",
        "        ids = [f\"X{i}\" for i in range(Dg.shape[0])]\n",
        "        m1, m2 = DistanceMatrix(Dg, ids=ids), DistanceMatrix(Dp, ids=ids)\n",
        "        r_m, p_m, _ = mantel(m1, m2, method='spearman', permutations=n_permutations)\n",
        "        results['mantel_r'] = float(r_m)\n",
        "        results['mantel_p'] = float(p_m)\n",
        "    except:\n",
        "        results['mantel_r'] = float(rho)\n",
        "        results['mantel_p'] = float(p)\n",
        "\n",
        "    # Retrieval metrics\n",
        "    n = Dg.shape[0]\n",
        "    Dg2, Dp2 = Dg.copy(), Dp.copy()\n",
        "    np.fill_diagonal(Dg2, np.inf)\n",
        "    np.fill_diagonal(Dp2, np.inf)\n",
        "\n",
        "    precisions = {k: [] for k in [5, 10, 20]}\n",
        "    for i in range(n):\n",
        "        rank_g = np.argsort(Dg2[i])\n",
        "        rank_p = np.argsort(Dp2[i])\n",
        "\n",
        "        for k in [5, 10, 20]:\n",
        "            if n > k:\n",
        "                overlap = len(set(rank_g[:k]) & set(rank_p[:k]))\n",
        "                precisions[k].append(overlap / k)\n",
        "\n",
        "    results['precision_at'] = {k: float(np.mean(v)) for k, v in precisions.items()}\n",
        "    results['random_baseline'] = {k: float(k/(n-1)) for k in [5, 10, 20]}\n",
        "\n",
        "    return results\n",
        "\n",
        "# =========================\n",
        "# Biological Analysis Functions\n",
        "# =========================\n",
        "\n",
        "def analyze_pathway_enrichment(high_concordance_pairs, low_concordance_pairs):\n",
        "    \"\"\"Analyze pathway enrichment of concordant vs discordant gene pairs\"\"\"\n",
        "    print(\"Analyzing pathway enrichment...\")\n",
        "\n",
        "    try:\n",
        "        gp = GProfiler(return_dataframe=True)\n",
        "\n",
        "        # Analyze high concordance genes\n",
        "        if len(high_concordance_pairs) > 5:\n",
        "            high_enrichment = gp.profile(\n",
        "                organism='hsapiens',\n",
        "                query=high_concordance_pairs,\n",
        "                sources=['GO:BP', 'KEGG', 'REACTOME']\n",
        "            )\n",
        "            print(f\"High concordance enrichment: {len(high_enrichment)} terms\")\n",
        "        else:\n",
        "            high_enrichment = pd.DataFrame()\n",
        "\n",
        "        # Analyze low concordance genes\n",
        "        if len(low_concordance_pairs) > 5:\n",
        "            low_enrichment = gp.profile(\n",
        "                organism='hsapiens',\n",
        "                query=low_concordance_pairs,\n",
        "                sources=['GO:BP', 'KEGG', 'REACTOME']\n",
        "            )\n",
        "            print(f\"Low concordance enrichment: {len(low_enrichment)} terms\")\n",
        "        else:\n",
        "            low_enrichment = pd.DataFrame()\n",
        "\n",
        "        return high_enrichment, low_enrichment\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Pathway analysis failed: {e}\")\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "def identify_concordant_gene_pairs(Dg, Dp, guide_names, percentile_threshold=90):\n",
        "    \"\"\"Identify gene pairs with high/low embedding-perturbation concordance\"\"\"\n",
        "    iu = np.triu_indices_from(Dg, k=1)\n",
        "    dist_g, dist_p = Dg[iu], Dp[iu]\n",
        "\n",
        "    # Compute concordance (negative of absolute difference in ranks)\n",
        "    rank_g = np.argsort(np.argsort(dist_g))\n",
        "    rank_p = np.argsort(np.argsort(dist_p))\n",
        "    concordance = -np.abs(rank_g - rank_p)\n",
        "\n",
        "    # Get high and low concordance pairs\n",
        "    high_threshold = np.percentile(concordance, percentile_threshold)\n",
        "    low_threshold = np.percentile(concordance, 100 - percentile_threshold)\n",
        "\n",
        "    high_idx = np.where(concordance >= high_threshold)[0]\n",
        "    low_idx = np.where(concordance <= low_threshold)[0]\n",
        "\n",
        "    # Extract gene pairs\n",
        "    row_idx, col_idx = iu\n",
        "    high_pairs = [(guide_names[row_idx[i]], guide_names[col_idx[i]]) for i in high_idx]\n",
        "    low_pairs = [(guide_names[row_idx[i]], guide_names[col_idx[i]]) for i in low_idx]\n",
        "\n",
        "    # Flatten to unique genes\n",
        "    high_genes = list(set([g for pair in high_pairs for g in pair]))\n",
        "    low_genes = list(set([g for pair in low_pairs for g in pair]))\n",
        "\n",
        "    print(f\"High concordance pairs: {len(high_pairs)} ({len(high_genes)} unique genes)\")\n",
        "    print(f\"Low concordance pairs: {len(low_pairs)} ({len(low_genes)} unique genes)\")\n",
        "\n",
        "    return high_genes, low_genes, high_pairs, low_pairs\n",
        "\n",
        "def supervised_prediction_analysis(Dg, Dp):\n",
        "    \"\"\"Test supervised learning for predicting perturbation similarity\"\"\"\n",
        "    print(\"Running supervised prediction analysis...\")\n",
        "\n",
        "    # Prepare data: embedding distances -> perturbation distances\n",
        "    iu = np.triu_indices_from(Dg, k=1)\n",
        "    X = Dg[iu].reshape(-1, 1)  # Embedding distances as features\n",
        "    y = Dp[iu]  # Perturbation distances as targets\n",
        "\n",
        "    # Random Forest regression\n",
        "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    scores = cross_val_score(rf, X, y, cv=5, scoring='r2')\n",
        "\n",
        "    # Fit for feature importance\n",
        "    rf.fit(X, y)\n",
        "\n",
        "    results = {\n",
        "        'cv_r2_mean': float(np.mean(scores)),\n",
        "        'cv_r2_std': float(np.std(scores)),\n",
        "        'train_r2': float(rf.score(X, y))\n",
        "    }\n",
        "\n",
        "    print(f\"Supervised prediction R²: {results['cv_r2_mean']:.4f} ± {results['cv_r2_std']:.4f}\")\n",
        "    return results\n",
        "\n",
        "# =========================\n",
        "# Main Enhanced Analysis\n",
        "# =========================\n",
        "\n",
        "def run_enhanced_analysis():\n",
        "    \"\"\"Run comprehensive enhanced analysis\"\"\"\n",
        "    print(\"=== ENHANCED GENE EMBEDDING VALIDATION ===\\n\")\n",
        "\n",
        "    # Load data\n",
        "    adata = load_gse133344()\n",
        "    adata = categorize_perturbations(adata)\n",
        "    generain_emb = load_generain_embeddings(FILES['generain'])\n",
        "\n",
        "    # Create pseudobulk and effects with optional filtering\n",
        "    pseudobulk_df, meta_df = make_gemgroup_matched_pseudobulk(adata, min_cells=20)\n",
        "\n",
        "    # Test with and without effect filtering\n",
        "    effects_all = compute_gemgroup_matched_effects(pseudobulk_df, meta_df, effect_threshold=None)\n",
        "    effects_strong = compute_gemgroup_matched_effects(pseudobulk_df, meta_df, effect_threshold=2.0)\n",
        "\n",
        "    # Create target combinations mapping\n",
        "    target_combinations = {}\n",
        "    for _, row in meta_df.iterrows():\n",
        "        guide = row['guide_identity']\n",
        "        targets = row['targets']\n",
        "        if guide in effects_all.index:  # Only include guides with computed effects\n",
        "            target_combinations[guide] = targets\n",
        "\n",
        "    print(f\"Target combinations for analysis: {len(target_combinations)}\")\n",
        "\n",
        "    # Create embeddings with different methods\n",
        "    embedding_methods = ['average', 'sum']\n",
        "    embedding_results = {}\n",
        "\n",
        "    for method in embedding_methods:\n",
        "        embedding_results[method] = create_combined_embeddings_enhanced(\n",
        "            target_combinations, generain_emb, method=method)\n",
        "\n",
        "    # Create random baseline\n",
        "    embedding_results['random'] = create_random_baseline_embeddings(target_combinations)\n",
        "\n",
        "    # Analysis results storage\n",
        "    analysis_results = {\n",
        "        'all_effects': {},\n",
        "        'strong_effects': {},\n",
        "        'cross_validation': {},\n",
        "        'biological_analysis': {}\n",
        "    }\n",
        "\n",
        "    # Test each combination of effects and embeddings\n",
        "    for effect_name, effects_df in [('all', effects_all), ('strong', effects_strong)]:\n",
        "        print(f\"\\n--- Analysis with {effect_name} effects ({len(effects_df)} guides) ---\")\n",
        "\n",
        "        effect_results = {}\n",
        "\n",
        "        for emb_method, embeddings_df in embedding_results.items():\n",
        "            print(f\"\\nTesting {emb_method} embeddings...\")\n",
        "\n",
        "            # Align datasets\n",
        "            common_guides = list(set(effects_df.index) & set(embeddings_df.index))\n",
        "            print(f\"Common guides: {len(common_guides)}\")\n",
        "\n",
        "            if len(common_guides) < 20:\n",
        "                continue\n",
        "\n",
        "            effects_aligned = effects_df.loc[common_guides]\n",
        "            embeddings_aligned = embeddings_df.loc[common_guides]\n",
        "\n",
        "            # Compute distances\n",
        "            Dp = pairwise_cosine_robust(effects_aligned.values)\n",
        "            Dg = pairwise_cosine_robust(embeddings_aligned.values)\n",
        "\n",
        "            # Enhanced metrics\n",
        "            metrics = compute_enhanced_metrics(Dg, Dp, n_permutations=999)\n",
        "            metrics['n_guides'] = len(common_guides)\n",
        "\n",
        "            # Supervised prediction (only for non-random)\n",
        "            if emb_method != 'random':\n",
        "                pred_results = supervised_prediction_analysis(Dg, Dp)\n",
        "                metrics.update(pred_results)\n",
        "\n",
        "            effect_results[emb_method] = metrics\n",
        "\n",
        "            print(f\"  Correlation: ρ = {metrics['spearman_rho']:.3f}, p = {metrics['spearman_p']:.2e}\")\n",
        "            print(f\"  Precision@10: {metrics['precision_at'][10]:.3f} vs {metrics['random_baseline'][10]:.3f} random\")\n",
        "\n",
        "            # Biological analysis (only for best non-random method)\n",
        "            if emb_method == 'average' and effect_name == 'strong':\n",
        "                try:\n",
        "                    high_genes, low_genes, high_pairs, low_pairs = identify_concordant_gene_pairs(\n",
        "                        Dg, Dp, common_guides, percentile_threshold=90)\n",
        "\n",
        "                    high_enrichment, low_enrichment = analyze_pathway_enrichment(high_genes, low_genes)\n",
        "\n",
        "                    analysis_results['biological_analysis'] = {\n",
        "                        'high_concordance_genes': high_genes[:20],  # Top 20 for storage\n",
        "                        'low_concordance_genes': low_genes[:20],\n",
        "                        'high_enrichment_terms': len(high_enrichment),\n",
        "                        'low_enrichment_terms': len(low_enrichment)\n",
        "                    }\n",
        "                except Exception as e:\n",
        "                    print(f\"Biological analysis failed: {e}\")\n",
        "\n",
        "        analysis_results[f'{effect_name}_effects'] = effect_results\n",
        "\n",
        "    # Cross-validation analysis\n",
        "    print(f\"\\n--- Cross-validation by gemgroups ---\")\n",
        "    train_gemgroups = [1, 2, 3, 4, 5, 6]\n",
        "    test_gemgroups = [7, 8]\n",
        "\n",
        "    cv_results = {}\n",
        "    for emb_method, embeddings_df in embedding_results.items():\n",
        "        if emb_method == 'random':\n",
        "            continue\n",
        "\n",
        "        cv_result = cross_validate_by_gemgroup(\n",
        "            effects_all, meta_df, embeddings_df, train_gemgroups, test_gemgroups)\n",
        "\n",
        "        if cv_result:\n",
        "            cv_results[emb_method] = cv_result\n",
        "            print(f\"  {emb_method}: ρ = {cv_result['spearman_rho']:.3f}, p = {cv_result['spearman_p']:.2e}\")\n",
        "\n",
        "    analysis_results['cross_validation'] = cv_results\n",
        "\n",
        "    # Find best method overall\n",
        "    best_method = None\n",
        "    best_rho = -1\n",
        "\n",
        "    for method, results in analysis_results['strong_effects'].items():\n",
        "        if method != 'random' and results['spearman_rho'] > best_rho:\n",
        "            best_rho = results['spearman_rho']\n",
        "            best_method = method\n",
        "\n",
        "    # Generate summary plot\n",
        "    create_summary_plots(analysis_results, best_method)\n",
        "\n",
        "    # Save comprehensive results\n",
        "    final_results = {\n",
        "        'analysis_type': 'enhanced_comprehensive',\n",
        "        'best_method': best_method,\n",
        "        'improvements_applied': [\n",
        "            'effect_magnitude_filtering',\n",
        "            'cross_validation_by_gemgroups',\n",
        "            'multiple_embedding_methods',\n",
        "            'random_baseline_comparison',\n",
        "            'biological_pathway_analysis',\n",
        "            'supervised_prediction_analysis'\n",
        "        ],\n",
        "        'results': analysis_results\n",
        "    }\n",
        "\n",
        "    with open(\"results/enhanced_analysis_results.json\", \"w\") as f:\n",
        "        json.dump(final_results, f, indent=2)\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\n=== ENHANCED ANALYSIS SUMMARY ===\")\n",
        "    print(f\"Best embedding method: {best_method}\")\n",
        "\n",
        "    if best_method and best_method in analysis_results['strong_effects']:\n",
        "        best_results = analysis_results['strong_effects'][best_method]\n",
        "        print(f\"Strong effects analysis:\")\n",
        "        print(f\"  Guides analyzed: {best_results['n_guides']}\")\n",
        "        print(f\"  Correlation: ρ = {best_results['spearman_rho']:.3f}, p = {best_results['spearman_p']:.2e}\")\n",
        "        print(f\"  Precision@10: {best_results['precision_at'][10]:.3f} vs {best_results['random_baseline'][10]:.3f} random\")\n",
        "        print(f\"  Improvement over random: {best_results['precision_at'][10] / best_results['random_baseline'][10]:.1f}x\")\n",
        "\n",
        "        if 'cv_r2_mean' in best_results:\n",
        "            print(f\"  Supervised prediction R²: {best_results['cv_r2_mean']:.4f}\")\n",
        "\n",
        "    if cv_results and best_method in cv_results:\n",
        "        print(f\"Cross-validation:\")\n",
        "        print(f\"  ρ = {cv_results[best_method]['spearman_rho']:.3f}, p = {cv_results[best_method]['spearman_p']:.2e}\")\n",
        "\n",
        "    return final_results\n",
        "\n",
        "def create_summary_plots(results, best_method):\n",
        "    \"\"\"Create comprehensive summary plots\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "    # Plot 1: Method comparison for strong effects\n",
        "    if 'strong_effects' in results:\n",
        "        methods = []\n",
        "        rhos = []\n",
        "        precisions = []\n",
        "\n",
        "        for method, res in results['strong_effects'].items():\n",
        "            methods.append(method)\n",
        "            rhos.append(res['spearman_rho'])\n",
        "            precisions.append(res['precision_at'][10])\n",
        "\n",
        "        axes[0,0].bar(methods, rhos)\n",
        "        axes[0,0].set_title('Correlation by Method')\n",
        "        axes[0,0].set_ylabel('Spearman ρ')\n",
        "        axes[0,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        axes[0,1].bar(methods, precisions)\n",
        "        axes[0,1].set_title('Precision@10 by Method')\n",
        "        axes[0,1].set_ylabel('Precision@10')\n",
        "        axes[0,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Plot 2: All vs Strong effects comparison\n",
        "    if best_method and best_method in results.get('all_effects', {}) and best_method in results.get('strong_effects', {}):\n",
        "        categories = ['All Effects', 'Strong Effects']\n",
        "        rhos = [\n",
        "            results['all_effects'][best_method]['spearman_rho'],\n",
        "            results['strong_effects'][best_method]['spearman_rho']\n",
        "        ]\n",
        "\n",
        "        axes[1,0].bar(categories, rhos)\n",
        "        axes[1,0].set_title(f'Effect Filtering Impact ({best_method})')\n",
        "        axes[1,0].set_ylabel('Spearman ρ')\n",
        "\n",
        "    # Plot 3: Cross-validation results\n",
        "    if 'cross_validation' in results and results['cross_validation']:\n",
        "        cv_methods = list(results['cross_validation'].keys())\n",
        "        cv_rhos = [results['cross_validation'][m]['spearman_rho'] for m in cv_methods]\n",
        "\n",
        "        axes[1,1].bar(cv_methods, cv_rhos)\n",
        "        axes[1,1].set_title('Cross-Validation Results')\n",
        "        axes[1,1].set_ylabel('Spearman ρ')\n",
        "        axes[1,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"figs/enhanced_analysis_summary.png\", dpi=200, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# =========================\n",
        "# Run Enhanced Analysis\n",
        "# =========================\n",
        "\n",
        "print(\"Enhanced analysis pipeline ready.\")\n",
        "print(\"Execute: results = run_enhanced_analysis()\")\n",
        "\n",
        "# Uncomment to run immediately:\n",
        "results = run_enhanced_analysis()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqARxBErp0Oc",
        "outputId": "ea0922a2-04ad-4788-827c-afe740e21679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m1.7/2.1 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.9/169.9 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.4/204.4 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.2/58.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.4/276.4 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m113.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n",
            "Enhanced analysis pipeline ready.\n",
            "Execute: results = run_enhanced_analysis()\n",
            "=== ENHANCED GENE EMBEDDING VALIDATION ===\n",
            "\n",
            "Loading GSE133344...\n",
            "Loaded: 111445 cells × 33694 genes | gemgroups: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8)]\n",
            "Perturbation type distribution:\n",
            "perturbation_type\n",
            "single     57831\n",
            "dual       41759\n",
            "control    11855\n",
            "Name: count, dtype: int64\n",
            "Loading GeneRAIN vectors from: /content/drive/MyDrive/dataset-gene-embed/GeneRAIN-vec.200d.txt\n",
            "GeneRAIN loaded: 31769 genes × 200 dims\n",
            "Creating gemgroup-matched pseudobulks...\n",
            "Created pseudobulk for 2037 guide-gemgroup combinations\n",
            "Computing gemgroup-matched effects...\n",
            "Found 4 control guides\n",
            "Computed effects for 268 guides\n",
            "Computing gemgroup-matched effects...\n",
            "Found 4 control guides\n",
            "Filtered to 268 guides with strong effects (|z| > 2.0)\n",
            "Computed effects for 268 guides\n",
            "Target combinations for analysis: 268\n",
            "Missing genes in embeddings: 4 unique genes\n",
            "Created average embeddings: 262 combinations\n",
            "Missing genes in embeddings: 4 unique genes\n",
            "Created sum embeddings: 262 combinations\n",
            "Created random baseline: 268 combinations, 200 dimensions\n",
            "\n",
            "--- Analysis with all effects (268 guides) ---\n",
            "\n",
            "Testing average embeddings...\n",
            "Common guides: 262\n",
            "Running supervised prediction analysis...\n",
            "Supervised prediction R²: -0.0076 ± 0.2051\n",
            "  Correlation: ρ = 0.080, p = 2.97e-49\n",
            "  Precision@10: 0.310 vs 0.038 random\n",
            "\n",
            "Testing sum embeddings...\n",
            "Common guides: 262\n",
            "Running supervised prediction analysis...\n",
            "Supervised prediction R²: -0.0076 ± 0.2051\n",
            "  Correlation: ρ = 0.080, p = 2.97e-49\n",
            "  Precision@10: 0.310 vs 0.038 random\n",
            "\n",
            "Testing random embeddings...\n",
            "Common guides: 268\n",
            "  Correlation: ρ = 0.002, p = 7.60e-01\n",
            "  Precision@10: 0.042 vs 0.037 random\n",
            "\n",
            "--- Analysis with strong effects (268 guides) ---\n",
            "\n",
            "Testing average embeddings...\n",
            "Common guides: 262\n",
            "Running supervised prediction analysis...\n",
            "Supervised prediction R²: -0.0076 ± 0.2051\n",
            "  Correlation: ρ = 0.080, p = 2.97e-49\n",
            "  Precision@10: 0.310 vs 0.038 random\n",
            "High concordance pairs: 3422 (262 unique genes)\n",
            "Low concordance pairs: 3420 (262 unique genes)\n",
            "Analyzing pathway enrichment...\n",
            "High concordance enrichment: 0 terms\n",
            "Low concordance enrichment: 0 terms\n",
            "\n",
            "Testing sum embeddings...\n",
            "Common guides: 262\n",
            "Running supervised prediction analysis...\n",
            "Supervised prediction R²: -0.0076 ± 0.2051\n",
            "  Correlation: ρ = 0.080, p = 2.97e-49\n",
            "  Precision@10: 0.310 vs 0.038 random\n",
            "\n",
            "Testing random embeddings...\n",
            "Common guides: 268\n",
            "  Correlation: ρ = 0.002, p = 7.60e-01\n",
            "  Precision@10: 0.042 vs 0.037 random\n",
            "\n",
            "--- Cross-validation by gemgroups ---\n",
            "Cross-validation: train on gemgroups [1, 2, 3, 4, 5, 6], test on [7, 8]\n",
            "Guides in both train/test: 252\n",
            "  average: ρ = 0.079, p = 6.42e-45\n",
            "Cross-validation: train on gemgroups [1, 2, 3, 4, 5, 6], test on [7, 8]\n",
            "Guides in both train/test: 252\n",
            "  sum: ρ = 0.079, p = 6.42e-45\n",
            "\n",
            "=== ENHANCED ANALYSIS SUMMARY ===\n",
            "Best embedding method: average\n",
            "Strong effects analysis:\n",
            "  Guides analyzed: 262\n",
            "  Correlation: ρ = 0.080, p = 2.97e-49\n",
            "  Precision@10: 0.310 vs 0.038 random\n",
            "  Improvement over random: 8.1x\n",
            "  Supervised prediction R²: -0.0076\n",
            "Cross-validation:\n",
            "  ρ = 0.079, p = 6.42e-45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# COMPLETE, CORRECTED, SINGLE-CELL PIPELINE\n",
        "# =========================\n",
        "\n",
        "!pip -q install scanpy anndata scikit-bio pingouin scikit-learn matplotlib pandas numpy scipy statsmodels\n",
        "\n",
        "import os, json, gzip, warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scanpy as sc\n",
        "import scipy.io\n",
        "from scipy import sparse\n",
        "from scipy.stats import spearmanr, mannwhitneyu, fisher_exact\n",
        "from skbio.stats.distance import mantel, DistanceMatrix\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cross_decomposition import CCA\n",
        "from scipy.linalg import orthogonal_procrustes\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ---------- Colab Drive ----------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ---------- Paths ----------\n",
        "BASE = \"/content/drive/MyDrive/dataset-gene-embed\"\n",
        "FILES = {\n",
        "    \"barcodes\": f\"{BASE}/GSE133344_filtered_barcodes.tsv.gz\",\n",
        "    \"cell_id\": f\"{BASE}/GSE133344_filtered_cell_identities.csv.gz\",\n",
        "    \"matrix\":   f\"{BASE}/GSE133344_filtered_matrix.mtx\",\n",
        "    \"genes\":    f\"{BASE}/GSE133344_filtered_genes.tsv\",\n",
        "    \"generain\": f\"{BASE}/GeneRAIN-vec.200d.txt\",\n",
        "}\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "os.makedirs(\"figs\", exist_ok=True)\n",
        "\n",
        "# =========================\n",
        "# Utilities (serialization, plotting helpers)\n",
        "# =========================\n",
        "\n",
        "def make_jsonable(obj):\n",
        "    \"\"\"Recursively convert NumPy/Pandas objects to JSON-safe types.\"\"\"\n",
        "    import numpy as _np\n",
        "    import pandas as _pd\n",
        "    if isinstance(obj, (int, float, str, bool)) or obj is None:\n",
        "        return obj\n",
        "    if isinstance(obj, (_np.integer, _np.floating)):\n",
        "        return obj.item()\n",
        "    if isinstance(obj, _np.ndarray):\n",
        "        return obj.tolist()\n",
        "    if isinstance(obj, (_pd.Series, _pd.Index)):\n",
        "        return obj.astype(object).tolist()\n",
        "    if isinstance(obj, _pd.DataFrame):\n",
        "        return {\"__dataframe__\": True, \"columns\": obj.columns.tolist(),\n",
        "                \"index\": obj.index.astype(str).tolist(), \"data\": obj.astype(object).values.tolist()}\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: make_jsonable(v) for k, v in obj.items()}\n",
        "    if isinstance(obj, (list, tuple)):\n",
        "        return [make_jsonable(v) for v in obj]\n",
        "    # fallback\n",
        "    try:\n",
        "        return obj.__repr__()\n",
        "    except Exception:\n",
        "        return str(type(obj))\n",
        "\n",
        "def save_json(path, payload):\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(payload, f, indent=2, default=make_jsonable)\n",
        "    print(f\"Saved JSON: {path}\")\n",
        "\n",
        "# =========================\n",
        "# 1) Load data\n",
        "# =========================\n",
        "\n",
        "def load_gse133344():\n",
        "    print(\"Loading GSE133344...\")\n",
        "    genes = pd.read_csv(FILES[\"genes\"], sep=\"\\t\", header=None, names=[\"ensembl_id\",\"gene_symbol\"])\n",
        "    with gzip.open(FILES[\"barcodes\"], \"rt\") as f:\n",
        "        barcodes = [ln.strip() for ln in f]\n",
        "    cell_id = pd.read_csv(FILES[\"cell_id\"], compression=\"gzip\")\n",
        "    X = scipy.io.mmread(FILES[\"matrix\"]).T.tocsr()\n",
        "\n",
        "    ad = sc.AnnData(X=X)\n",
        "    ad.obs_names = barcodes\n",
        "    ad.var_names = genes[\"gene_symbol\"].values\n",
        "    ad.var[\"ensembl_id\"] = genes[\"ensembl_id\"].values\n",
        "    ad.var_names_make_unique()\n",
        "\n",
        "    cell_id = cell_id.set_index(\"cell_barcode\")\n",
        "    common = list(set(ad.obs_names) & set(cell_id.index))\n",
        "    ad = ad[common].copy()\n",
        "    ad.obs = ad.obs.join(cell_id.loc[common])\n",
        "\n",
        "    print(f\"Loaded: {ad.n_obs} cells × {ad.n_vars} genes | gemgroups: {sorted(ad.obs['gemgroup'].unique())}\")\n",
        "    return ad\n",
        "\n",
        "def parse_combinatorial_targets(guide_identity):\n",
        "    if pd.isna(guide_identity):\n",
        "        return []\n",
        "    target_part = guide_identity.split('__')[0]\n",
        "    components = [c for c in target_part.split('_') if c and 'NegCtrl' not in c]\n",
        "    # dedupe keep order\n",
        "    seen, out = set(), []\n",
        "    for c in components:\n",
        "        if c not in seen:\n",
        "            seen.add(c); out.append(c)\n",
        "    return out\n",
        "\n",
        "def categorize_perturbations(ad):\n",
        "    ad.obs['parsed_targets'] = ad.obs['guide_identity'].apply(parse_combinatorial_targets)\n",
        "    ad.obs['n_targets'] = ad.obs['parsed_targets'].apply(len)\n",
        "    ad.obs['perturbation_type'] = 'unknown'\n",
        "    ad.obs.loc[ad.obs['n_targets'] == 0, 'perturbation_type'] = 'control'\n",
        "    ad.obs.loc[ad.obs['n_targets'] == 1, 'perturbation_type'] = 'single'\n",
        "    ad.obs.loc[ad.obs['n_targets'] == 2, 'perturbation_type'] = 'dual'\n",
        "    ad.obs.loc[ad.obs['n_targets'] > 2, 'perturbation_type'] = 'multi'\n",
        "    print(\"Perturbation type distribution:\")\n",
        "    print(ad.obs['perturbation_type'].value_counts())\n",
        "    return ad\n",
        "\n",
        "def load_generain_embeddings(path: str):\n",
        "    print(f\"Loading GeneRAIN vectors from: {path}\")\n",
        "    genes, embeds = [], []\n",
        "    with open(path, \"r\") as f:\n",
        "        _ = f.readline().strip()  # header\n",
        "        for ln in f:\n",
        "            parts = ln.strip().split()\n",
        "            if len(parts) < 2: continue\n",
        "            genes.append(parts[0]); embeds.append([float(x) for x in parts[1:]])\n",
        "    df = pd.DataFrame(embeds, index=pd.Index(genes, name=\"gene\"))\n",
        "    df.columns = [f\"generain_{i}\" for i in range(df.shape[1])]\n",
        "    if df.index.duplicated().any():\n",
        "        df = df[~df.index.duplicated(keep=\"first\")]\n",
        "    print(f\"GeneRAIN loaded: {df.shape[0]} genes × {df.shape[1]} dims\")\n",
        "    return df\n",
        "\n",
        "# =========================\n",
        "# 2) Pseudobulk & effects\n",
        "# =========================\n",
        "\n",
        "def make_gemgroup_matched_combinatorial_pseudobulk(ad, min_cells=20):\n",
        "    print(\"Creating gemgroup-matched pseudobulks...\")\n",
        "    groups = ad.obs.groupby(['guide_identity', 'gemgroup']).indices\n",
        "    data, meta = [], []\n",
        "    X = ad.X; is_sparse = sparse.issparse(X)\n",
        "    for (guide, gemgroup), idx in groups.items():\n",
        "        if len(idx) < min_cells: continue\n",
        "        summed = np.array(X[idx].sum(axis=0)).ravel() if is_sparse else X[idx].sum(axis=0)\n",
        "        data.append(summed)\n",
        "        meta.append({\n",
        "            'guide_identity': guide, 'gemgroup': gemgroup, 'n_cells': len(idx),\n",
        "            'targets': ad.obs.loc[ad.obs_names[idx[0]], 'parsed_targets'],\n",
        "            'perturbation_type': ad.obs.loc[ad.obs_names[idx[0]], 'perturbation_type']\n",
        "        })\n",
        "    pb = pd.DataFrame(data, columns=ad.var_names)\n",
        "    pb.index = pd.MultiIndex.from_frame(pd.DataFrame(meta)[['guide_identity', 'gemgroup']])\n",
        "    meta_df = pd.DataFrame(meta)\n",
        "    print(f\"Created pseudobulk for {len(pb)} guide-gemgroup combinations\")\n",
        "    return pb, meta_df\n",
        "\n",
        "def compute_gemgroup_matched_combinatorial_effects(pseudobulk_df, meta_df):\n",
        "    print(\"Computing gemgroup-matched effects...\")\n",
        "    lib = pseudobulk_df.sum(axis=1)\n",
        "    cpm = pseudobulk_df.div(lib, axis=0) * 1e6\n",
        "    logcpm = np.log1p(cpm)\n",
        "\n",
        "    control_guides = set(meta_df[meta_df['perturbation_type'] == 'control']['guide_identity'])\n",
        "    print(f\"Found {len(control_guides)} control guides\")\n",
        "    all_guides = sorted(set(meta_df['guide_identity']))\n",
        "    target_guides = [g for g in all_guides if g not in control_guides]\n",
        "\n",
        "    effects = {}\n",
        "    for guide in target_guides:\n",
        "        try:\n",
        "            gp = logcpm.loc[guide]\n",
        "            if isinstance(gp, pd.Series):\n",
        "                gp = pd.DataFrame([gp.values], index=[gp.name], columns=logcpm.columns)\n",
        "            per_group = []\n",
        "            for gg in gp.index:\n",
        "                ctrls = []\n",
        "                for cg in control_guides:\n",
        "                    try: ctrls.append(logcpm.loc[(cg, gg)])\n",
        "                    except KeyError: continue\n",
        "                if not ctrls: continue\n",
        "                ctrl_mean = pd.concat(ctrls, axis=1).mean(axis=1) if len(ctrls) > 1 else ctrls[0]\n",
        "                per_group.append(gp.loc[gg] - ctrl_mean)\n",
        "            if per_group:\n",
        "                mean_effect = pd.concat(per_group, axis=1).mean(axis=1) if len(per_group)>1 else per_group[0]\n",
        "                effects[guide] = mean_effect.values\n",
        "        except KeyError:\n",
        "            continue\n",
        "    eff = pd.DataFrame.from_dict(effects, orient='index', columns=logcpm.columns)\n",
        "    mu, sd = eff.mean(axis=0), eff.std(axis=0, ddof=0).replace(0, 1.0)\n",
        "    eff_z = eff.subtract(mu, axis=1).divide(sd, axis=1)\n",
        "    eff_z = eff_z.replace([np.inf, -np.inf], 0).fillna(0)\n",
        "    print(f\"Computed effects for {len(eff_z)} guides\")\n",
        "    return eff_z\n",
        "\n",
        "# =========================\n",
        "# 3) Embeddings (combine genes for combos)\n",
        "# =========================\n",
        "\n",
        "def create_combined_embeddings(target_combinations, generain_embeddings, method='average'):\n",
        "    combined, missing = {}, set()\n",
        "    for combo_name, gene_list in target_combinations.items():\n",
        "        if len(gene_list) == 0:\n",
        "            combined[combo_name] = np.zeros(generain_embeddings.shape[1]); continue\n",
        "        available = [g for g in gene_list if g in generain_embeddings.index]\n",
        "        missing.update(set(gene_list) - set(available))\n",
        "        if not available: continue\n",
        "        mat = np.stack([generain_embeddings.loc[g].values for g in available])\n",
        "        if method == 'sum': vec = np.sum(mat, axis=0)\n",
        "        elif method == 'hadamard': vec = np.prod(mat, axis=0)\n",
        "        else: vec = np.mean(mat, axis=0)\n",
        "        combined[combo_name] = vec\n",
        "    if missing:\n",
        "        print(f\"Missing genes in embeddings: {len(missing)} unique genes\")\n",
        "    df = pd.DataFrame.from_dict(combined, orient='index')\n",
        "    print(f\"Created {method} embeddings: {df.shape[0]} combinations\")\n",
        "    return df\n",
        "\n",
        "# =========================\n",
        "# 4) Distance + metrics\n",
        "# =========================\n",
        "\n",
        "def pairwise_cosine(M):\n",
        "    M = np.nan_to_num(M, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "    norms = np.linalg.norm(M, axis=1)\n",
        "    if np.any(norms == 0):\n",
        "        M[norms==0] += np.random.normal(0, 1e-6, size=(np.sum(norms==0), M.shape[1]))\n",
        "    D = pairwise_distances(M, metric='cosine')\n",
        "    np.fill_diagonal(D, 0.0)\n",
        "    return np.nan_to_num(D, nan=1.0, posinf=1.0, neginf=0.0)\n",
        "\n",
        "def mantel_and_spearman(D1, D2, n_perms=999):\n",
        "    iu = np.triu_indices_from(D1, k=1)\n",
        "    x, y = D1[iu], D2[iu]\n",
        "    mask = ~(np.isnan(x) | np.isnan(y) | np.isinf(x) | np.isinf(y))\n",
        "    x, y = x[mask], y[mask]\n",
        "    if len(x)==0: return 0.0, 1.0, 0.0, 1.0\n",
        "    rho, p = spearmanr(x, y)\n",
        "    try:\n",
        "        ids = [f\"X{i}\" for i in range(D1.shape[0])]\n",
        "        r_m, p_m, _ = mantel(DistanceMatrix(D1, ids=ids),\n",
        "                             DistanceMatrix(D2, ids=ids),\n",
        "                             method='spearman', permutations=n_perms)\n",
        "    except Exception:\n",
        "        r_m, p_m = rho, p\n",
        "    return float(rho), float(p), float(r_m), float(p_m)\n",
        "\n",
        "def retrieval_metrics(Dg, Dp, ks=(5,10,20)):\n",
        "    n = Dg.shape[0]\n",
        "    Dg2, Dp2 = Dg.copy(), Dp.copy()\n",
        "    np.fill_diagonal(Dg2, np.inf); np.fill_diagonal(Dp2, np.inf)\n",
        "    precisions = {k: [] for k in ks}\n",
        "    for i in range(n):\n",
        "        rg, rp = np.argsort(Dg2[i]), np.argsort(Dp2[i])\n",
        "        for k in ks:\n",
        "            if n>k:\n",
        "                precisions[k].append(len(set(rg[:k]) & set(rp[:k]))/k)\n",
        "    return {\"precision_at\": {k: float(np.mean(v)) for k,v in precisions.items()}}\n",
        "\n",
        "# =========================\n",
        "# 5) Biology helpers (families, per-gene, plots)\n",
        "# =========================\n",
        "\n",
        "def load_curated_gene_families():\n",
        "    # Curated, concise sets—enough for enrichment checks\n",
        "    TF = {'JUN','FOS','MYC','MAX','TP53','STAT1','STAT3','NFKB1','RELA','E2F1','E2F2','GATA1','GATA2',\n",
        "          'KLF1','CEBPA','RUNX1','ETS1','ETS2','FOXA1','FOXO1','FOXO3','LEF1','TCF7'}\n",
        "    kinase = {'AKT1','AKT2','PIK3CA','PIK3CB','MTOR','MAPK1','MAPK3','MAPK8','MAPK14','MAP2K1','MAP2K2',\n",
        "              'CDK1','CDK2','CDK4','CDK6','GSK3B','SRC','JAK1','JAK2','EGFR'}\n",
        "    chromatin = {'HDAC1','HDAC2','SIRT1','EP300','CREBBP','KAT2A','KAT2B','BRD4','CHD1','SMARCA4','ARID1A'}\n",
        "    metabolic = {'PFKM','PKM','LDHA','G6PD','GAPDH','ENO1','HK2','FASN','ACACA','CPT1A','ACLY','SDHA'}\n",
        "    return {'TF': TF, 'kinase': kinase, 'chromatin': chromatin, 'metabolic': metabolic}\n",
        "\n",
        "def assign_gene_families(genes, fams):\n",
        "    df = pd.DataFrame(index=genes)\n",
        "    df['gene_family'] = 'other'\n",
        "    df.loc[[g for g in genes if g.upper() in fams['TF']], 'gene_family'] = 'transcription_factor'\n",
        "    df.loc[[g for g in genes if g.upper() in fams['kinase']], 'gene_family'] = 'kinase'\n",
        "    df.loc[[g for g in genes if g.upper() in fams['chromatin']], 'gene_family'] = 'chromatin_regulator'\n",
        "    df.loc[[g for g in genes if g.upper() in fams['metabolic']], 'gene_family'] = 'metabolic_enzyme'\n",
        "    return df\n",
        "\n",
        "def compute_per_gene_concordance(Dg, Dp, gene_names):\n",
        "    print(\"Computing per-gene concordance...\")\n",
        "    n = len(gene_names)\n",
        "    rows = []\n",
        "    for i,g in enumerate(gene_names):\n",
        "        mask = np.ones(n, dtype=bool); mask[i]=False\n",
        "        r, p = spearmanr(Dg[i,mask], Dp[i,mask])\n",
        "        if np.isnan(r): r, p = 0.0, 1.0\n",
        "        rows.append((g, float(r), float(p)))\n",
        "    out = pd.DataFrame(rows, columns=['gene','rho','p']).set_index('gene')\n",
        "    out['q'] = multipletests(out['p'], method='fdr_bh')[1]\n",
        "    out = out.sort_values('rho', ascending=False)\n",
        "    print(f\"Per-gene: mean ρ={out['rho'].mean():.3f}, max ρ={out['rho'].max():.3f}, sig(q<0.05)={(out['q']<0.05).sum()}\")\n",
        "    return out\n",
        "\n",
        "def create_procrustes_alignment_plots(embeddings_df, effects_df, label=\"all\"):\n",
        "    print(\"Creating Procrustes alignment visualization...\")\n",
        "    pca_emb, pca_pert = PCA(n_components=2, random_state=42), PCA(n_components=2, random_state=42)\n",
        "    X_emb = pca_emb.fit_transform(embeddings_df.values)\n",
        "    X_pert = pca_pert.fit_transform(effects_df.values)\n",
        "    R,_ = orthogonal_procrustes(X_emb, X_pert)\n",
        "    X_al = X_emb @ R\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 4.5))\n",
        "    for ax, xi, yi, ttl in [(axes[0], X_al[:,0], X_pert[:,0], \"PC1\"),\n",
        "                            (axes[1], X_al[:,1], X_pert[:,1], \"PC2\")]:\n",
        "        r,p = spearmanr(xi, yi)\n",
        "        ax.scatter(xi, yi, s=25, alpha=0.7)\n",
        "        z = np.polyfit(xi, yi, 1); ax.plot(xi, np.poly1d(z)(xi), \"r--\", lw=1)\n",
        "        ax.set_title(f'{ttl} Alignment\\nρ = {r:.3f}, p = {p:.2e}')\n",
        "        ax.set_xlabel(f'Embedding {ttl} (aligned)'); ax.set_ylabel(f'Perturbation {ttl}'); ax.grid(True, alpha=0.3)\n",
        "    axes[2].scatter(X_al[:,0], X_al[:,1], s=20, alpha=0.6, label='Embedding (aligned)')\n",
        "    axes[2].scatter(X_pert[:,0], X_pert[:,1], s=20, alpha=0.6, label='Perturbation', marker='s')\n",
        "    axes[2].legend(); axes[2].set_title('2D Space Overlay'); axes[2].grid(True, alpha=0.3)\n",
        "    plt.tight_layout(); outpath = f\"figs/procrustes_{label}.png\"\n",
        "    plt.savefig(outpath, dpi=200, bbox_inches='tight'); plt.close()\n",
        "    return {\"pc_var_emb\": pca_emb.explained_variance_ratio_,\n",
        "            \"pc_var_pert\": pca_pert.explained_variance_ratio_,\n",
        "            \"plot_path\": outpath}\n",
        "\n",
        "# =========================\n",
        "# 6) Main analysis (global + per-gene + CCA)\n",
        "# =========================\n",
        "\n",
        "def run_complete_biological_analysis():\n",
        "    print(\"=\"*70); print(\"COMPLETE BIOLOGICAL GENE EMBEDDING ANALYSIS\"); print(\"=\"*70)\n",
        "\n",
        "    # Load\n",
        "    adata = load_gse133344()\n",
        "    adata = categorize_perturbations(adata)\n",
        "    generain = load_generain_embeddings(FILES['generain'])\n",
        "\n",
        "    # Pseudobulk + effects\n",
        "    pb, meta = make_gemgroup_matched_combinatorial_pseudobulk(adata, min_cells=20)\n",
        "    effects = compute_gemgroup_matched_combinatorial_effects(pb, meta)\n",
        "\n",
        "    # Target mapping\n",
        "    combos = {}\n",
        "    for _,row in meta.iterrows():\n",
        "        g = row['guide_identity']; t = row['targets']\n",
        "        if g in effects.index: combos[g] = t\n",
        "    print(f\"Target combinations for analysis: {len(combos)}\")\n",
        "\n",
        "    # Embeddings (avg & sum)\n",
        "    emb_by = {m: create_combined_embeddings(combos, generain, method=m) for m in ['average','sum']}\n",
        "\n",
        "    # Choose best by Spearman with perturbation distances\n",
        "    best_method, best_stat = None, -np.inf\n",
        "    method_stats = {}\n",
        "    for m, emb_df in emb_by.items():\n",
        "        common = sorted(set(effects.index) & set(emb_df.index))\n",
        "        print(f\"\\n--- Testing {m} ---\\nCommon guides: {len(common)}\")\n",
        "        if len(common) < 20: continue\n",
        "        Eff = effects.loc[common]; Emb = emb_df.loc[common]\n",
        "        Dp, Dg = pairwise_cosine(Eff.values), pairwise_cosine(Emb.values)\n",
        "        rho, p, r_m, p_m = mantel_and_spearman(Dg, Dp, n_perms=999)\n",
        "        retrieval = retrieval_metrics(Dg, Dp)\n",
        "        method_stats[m] = {\"n\": len(common), \"rho\": rho, \"p\": p,\n",
        "                           \"mantel_r\": r_m, \"mantel_p\": p_m,\n",
        "                           \"precision_at\": retrieval['precision_at']}\n",
        "        print(f\"Correlation: ρ={rho:.3f}, p={p:.2e} | P@10={retrieval['precision_at'].get(10, np.nan):.3f}\")\n",
        "        if rho > best_stat:\n",
        "            best_stat, best_method = rho, m\n",
        "\n",
        "    print(f\"\\nBest embedding method: {best_method}\")\n",
        "\n",
        "    # Align best for global reporting\n",
        "    best_emb = emb_by[best_method]\n",
        "    common_all = sorted(set(effects.index) & set(best_emb.index))\n",
        "    Eff_all, Emb_all = effects.loc[common_all], best_emb.loc[common_all]\n",
        "    Dp_all, Dg_all = pairwise_cosine(Eff_all.values), pairwise_cosine(Emb_all.values)\n",
        "    iu = np.triu_indices_from(Dg_all, k=1); rho_global, p_global = spearmanr(Dg_all[iu], Dp_all[iu])\n",
        "    print(f\"Overall correlation: ρ = {rho_global:.3f}, p = {p_global:.2e}\")\n",
        "\n",
        "    # ---------- Per-gene analysis on SINGLE-GENE guides ----------\n",
        "    single_guides = [g for g in common_all if len(parse_combinatorial_targets(g))==1]\n",
        "    if len(single_guides) >= 20:\n",
        "        # map guide->gene name\n",
        "        gene_names = [parse_combinatorial_targets(g)[0] for g in single_guides]\n",
        "        Emb_sg = pd.DataFrame(Emb_all.loc[single_guides].values, index=gene_names, columns=Emb_all.columns)\n",
        "        Eff_sg = pd.DataFrame(Eff_all.loc[single_guides].values, index=gene_names, columns=Eff_all.columns)\n",
        "        # remove duplicates\n",
        "        Emb_sg, Eff_sg = Emb_sg[~Emb_sg.index.duplicated(keep='first')], Eff_sg[~Eff_sg.index.duplicated(keep='first')]\n",
        "        Dg_sg, Dp_sg = pairwise_cosine(Emb_sg.values), pairwise_cosine(Eff_sg.values)\n",
        "        per_gene = compute_per_gene_concordance(Dg_sg, Dp_sg, Emb_sg.index.tolist())\n",
        "\n",
        "        # gene families (meaningful now)\n",
        "        fams = load_curated_gene_families()\n",
        "        fam_labels = assign_gene_families(Emb_sg.index.tolist(), fams)\n",
        "\n",
        "        # enrichment among top 25%\n",
        "        thr = np.percentile(per_gene['rho'], 75)\n",
        "        top_genes = per_gene[per_gene['rho']>=thr].index\n",
        "        print(f\"\\nHigh-concordance genes (top 25%): {len(top_genes)}\")\n",
        "        # family enrichment quick check\n",
        "        for fam_name in ['transcription_factor','kinase','chromatin_regulator','metabolic_enzyme']:\n",
        "            fam_set = fam_labels[fam_labels['gene_family']==fam_name].index\n",
        "            a = len(set(top_genes) & set(fam_set))\n",
        "            b = len(fam_set) - a\n",
        "            c = len(set(top_genes) - set(fam_set))\n",
        "            d = len(Emb_sg.index) - a - b - c\n",
        "            if a+b >= 3 and c+d >= 3:\n",
        "                OR, p = fisher_exact([[a,b],[c,d]], alternative='greater')\n",
        "                print(f\"  {fam_name}: {a}/{a+b} high; OR={OR:.2f}, p={p:.3f}\")\n",
        "\n",
        "        # CCA (2 comps)\n",
        "        cca = CCA(n_components=2, max_iter=500)\n",
        "        Xc, Yc = cca.fit_transform(Emb_sg.values, Eff_sg.values)\n",
        "        cca_r1, cca_p1 = spearmanr(Xc[:,0], Yc[:,0])\n",
        "        cca_r2, cca_p2 = spearmanr(Xc[:,1], Yc[:,1])\n",
        "        print(f\"\\nCCA: CC1 ρ={cca_r1:.3f} (p={cca_p1:.2e}), CC2 ρ={cca_r2:.3f} (p={cca_p2:.2e})\")\n",
        "\n",
        "        # Visuals\n",
        "        align_all = create_procrustes_alignment_plots(Emb_all, Eff_all, label=\"all_guides\")\n",
        "        align_sg  = create_procrustes_alignment_plots(Emb_sg, Eff_sg, label=\"single_genes\")\n",
        "\n",
        "        # Save top concordant barplot (family-colored)\n",
        "        topN = per_gene.head(20).index\n",
        "        colors = {'transcription_factor':'#e74c3c','kinase':'#3498db','chromatin_regulator':'#2ecc71','metabolic_enzyme':'#f39c12','other':'#95a5a6'}\n",
        "        fam_for_top = fam_labels.loc[topN, 'gene_family']\n",
        "        plt.figure(figsize=(10,8))\n",
        "        plt.barh(range(len(topN)), per_gene.loc[topN,'rho'].values,\n",
        "                 color=[colors[f] for f in fam_for_top])\n",
        "        plt.yticks(range(len(topN)), topN); plt.xlabel('Per-gene concordance (ρ)')\n",
        "        plt.title('Top 20 Most Concordant Genes (single-gene subset)')\n",
        "        plt.grid(axis='x', alpha=0.3)\n",
        "        plt.tight_layout(); plt.savefig(\"figs/top20_per_gene_concordance.png\", dpi=200, bbox_inches='tight'); plt.close()\n",
        "\n",
        "        # Package results\n",
        "        final = {\n",
        "            \"analysis_type\": \"complete_biological_analysis\",\n",
        "            \"best_method\": best_method,\n",
        "            \"overall\": {\"rho\": rho_global, \"p\": p_global,\n",
        "                        \"n_combinations\": len(common_all),\n",
        "                        \"method_stats\": method_stats},\n",
        "            \"single_gene\": {\n",
        "                \"n_genes\": len(Emb_sg),\n",
        "                \"per_gene_summary\": {\n",
        "                    \"mean_rho\": float(per_gene['rho'].mean()),\n",
        "                    \"max_rho\": float(per_gene['rho'].max()),\n",
        "                    \"n_sig_q<0.05\": int((per_gene['q']<0.05).sum())\n",
        "                },\n",
        "                \"top10\": per_gene.head(10)\n",
        "            },\n",
        "            \"cca\": {\"cc1_spearman\": float(cca_r1), \"cc1_p\": float(cca_p1),\n",
        "                    \"cc2_spearman\": float(cca_r2), \"cc2_p\": float(cca_p2)},\n",
        "            \"alignment_plots\": {\"all_guides\": align_all, \"single_genes\": align_sg}\n",
        "        }\n",
        "\n",
        "        save_json(\"results/complete_biological_analysis.json\", final)\n",
        "\n",
        "        # Return detailed data (for any follow-ups)\n",
        "        detailed = {\n",
        "            \"effects_aligned\": Eff_all, \"embeddings_aligned\": Emb_all,\n",
        "            \"effects_single\": Eff_sg,   \"embeddings_single\": Emb_sg,\n",
        "            \"Dg_all\": Dg_all, \"Dp_all\": Dp_all,\n",
        "            \"Dg_single\": Dg_sg, \"Dp_single\": Dp_sg,\n",
        "            \"per_gene_df\": per_gene, \"family_labels\": fam_labels\n",
        "        }\n",
        "        return final, detailed\n",
        "\n",
        "    else:\n",
        "        print(\"Not enough single-gene guides for per-gene analysis (need ≥20). Saving global only.\")\n",
        "        final = {\n",
        "            \"analysis_type\": \"complete_biological_analysis\",\n",
        "            \"best_method\": best_method,\n",
        "            \"overall\": {\"rho\": rho_global, \"p\": p_global,\n",
        "                        \"n_combinations\": len(common_all),\n",
        "                        \"method_stats\": method_stats}\n",
        "        }\n",
        "        save_json(\"results/complete_biological_analysis.json\", final)\n",
        "        return final, {\"effects_aligned\": Eff_all, \"embeddings_aligned\": Emb_all,\n",
        "                       \"Dg_all\": Dg_all, \"Dp_all\": Dp_all}\n",
        "\n",
        "# =========================\n",
        "# 7) (Optional) interaction & pathway extras based on single-gene output\n",
        "# =========================\n",
        "\n",
        "def cca_overlay_plot(Emb_sg, Eff_sg, per_gene_df, top_n=10):\n",
        "    cca = CCA(n_components=2, max_iter=500)\n",
        "    Xc, Yc = cca.fit_transform(Emb_sg.values, Eff_sg.values)\n",
        "    r1,p1 = spearmanr(Xc[:,0], Yc[:,0]); r2,p2 = spearmanr(Xc[:,1], Yc[:,1])\n",
        "    fig, axes = plt.subplots(1,3, figsize=(15,4.5))\n",
        "    for ax, xi, yi, ttl, r,p in [(axes[0], Xc[:,0], Yc[:,0], \"CC1\", r1,p1),\n",
        "                                 (axes[1], Xc[:,1], Yc[:,1], \"CC2\", r2,p2)]:\n",
        "        ax.scatter(xi, yi, s=25, alpha=0.6)\n",
        "        z = np.polyfit(xi, yi, 1); ax.plot(xi, np.poly1d(z)(xi), \"r--\", lw=1)\n",
        "        ax.set_title(f\"{ttl}: ρ={r:.3f}, p={p:.2e}\"); ax.grid(True, alpha=0.3)\n",
        "    axes[2].scatter(Xc[:,0], Xc[:,1], s=20, alpha=0.6, label=\"Embedding (CCA)\")\n",
        "    axes[2].scatter(Yc[:,0], Yc[:,1], s=20, alpha=0.6, label=\"Perturbation (CCA)\", marker=\"s\")\n",
        "    axes[2].legend(); axes[2].set_title(\"2D CCA Overlay\"); axes[2].grid(True, alpha=0.3)\n",
        "    plt.tight_layout(); plt.savefig(\"figs/cca_overlay.png\", dpi=200, bbox_inches='tight'); plt.close()\n",
        "\n",
        "    # label top genes on CC1\n",
        "    names = Emb_sg.index.tolist()\n",
        "    top = per_gene_df.head(top_n).index\n",
        "    plt.figure(figsize=(8,6)); plt.scatter(Xc[:,0], Yc[:,0], s=15, alpha=0.4, color='lightgray')\n",
        "    for g in top:\n",
        "        i = names.index(g)\n",
        "        plt.scatter(Xc[i,0], Yc[i,0], s=50, color='crimson')\n",
        "        plt.text(Xc[i,0], Yc[i,0], g, fontsize=8,\n",
        "                 bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.7))\n",
        "    plt.xlabel(\"Embedding CC1\"); plt.ylabel(\"Perturbation CC1\")\n",
        "    plt.title(f\"Top {top_n} Concordant Genes (CCA1)\"); plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout(); plt.savefig(\"figs/top_genes_cca_labeled.png\", dpi=200, bbox_inches='tight'); plt.close()\n",
        "\n",
        "# =========================\n",
        "# 8) Run\n",
        "# =========================\n",
        "\n",
        "print(\"Pipeline loaded. Running end-to-end...\")\n",
        "final_results, detailed_data = run_complete_biological_analysis()\n",
        "\n",
        "# Optional: richer CCA overlay with labels (runs only if single-gene data returned)\n",
        "if 'embeddings_single' in detailed_data:\n",
        "    cca_overlay_plot(detailed_data['embeddings_single'],\n",
        "                     detailed_data['effects_single'],\n",
        "                     detailed_data['per_gene_df'],\n",
        "                     top_n=10)\n",
        "\n",
        "print(\"\\nDone. Key files:\")\n",
        "print(\" - results/complete_biological_analysis.json\")\n",
        "print(\" - figs/procrustes_all_guides.png, figs/procrustes_single_genes.png\")\n",
        "print(\" - figs/top20_per_gene_concordance.png\")\n",
        "print(\" - figs/cca_overlay.png, figs/top_genes_cca_labeled.png (if single-gene analysis ran)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGHiLG7J5wUM",
        "outputId": "9c5a0120-1035-4b00-eb04-22291d0332f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Pipeline loaded. Running end-to-end...\n",
            "======================================================================\n",
            "COMPLETE BIOLOGICAL GENE EMBEDDING ANALYSIS\n",
            "======================================================================\n",
            "Loading GSE133344...\n",
            "Loaded: 111445 cells × 33694 genes | gemgroups: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8)]\n",
            "Perturbation type distribution:\n",
            "perturbation_type\n",
            "single     57831\n",
            "dual       41759\n",
            "control    11855\n",
            "Name: count, dtype: int64\n",
            "Loading GeneRAIN vectors from: /content/drive/MyDrive/dataset-gene-embed/GeneRAIN-vec.200d.txt\n",
            "GeneRAIN loaded: 31769 genes × 200 dims\n",
            "Creating gemgroup-matched pseudobulks...\n",
            "Created pseudobulk for 2037 guide-gemgroup combinations\n",
            "Computing gemgroup-matched effects...\n",
            "Found 4 control guides\n",
            "Computed effects for 268 guides\n",
            "Target combinations for analysis: 268\n",
            "Missing genes in embeddings: 4 unique genes\n",
            "Created average embeddings: 262 combinations\n",
            "Missing genes in embeddings: 4 unique genes\n",
            "Created sum embeddings: 262 combinations\n",
            "\n",
            "--- Testing average ---\n",
            "Common guides: 262\n",
            "Correlation: ρ=0.080, p=2.97e-49 | P@10=0.310\n",
            "\n",
            "--- Testing sum ---\n",
            "Common guides: 262\n",
            "Correlation: ρ=0.080, p=2.97e-49 | P@10=0.310\n",
            "\n",
            "Best embedding method: average\n",
            "Overall correlation: ρ = 0.080, p = 2.97e-49\n",
            "Computing per-gene concordance...\n",
            "Per-gene: mean ρ=0.039, max ρ=0.321, sig(q<0.05)=0\n",
            "\n",
            "High-concordance genes (top 25%): 25\n",
            "  transcription_factor: 2/5 high; OR=2.09, p=0.367\n",
            "\n",
            "CCA: CC1 ρ=0.968 (p=1.24e-60), CC2 ρ=1.000 (p=0.00e+00)\n",
            "Creating Procrustes alignment visualization...\n",
            "Creating Procrustes alignment visualization...\n",
            "Saved JSON: results/complete_biological_analysis.json\n",
            "\n",
            "Done. Key files:\n",
            " - results/complete_biological_analysis.json\n",
            " - figs/procrustes_all_guides.png, figs/procrustes_single_genes.png\n",
            " - figs/top20_per_gene_concordance.png\n",
            " - figs/cca_overlay.png, figs/top_genes_cca_labeled.png (if single-gene analysis ran)\n"
          ]
        }
      ]
    }
  ]
}