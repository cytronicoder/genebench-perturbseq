{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install scanpy anndata scikit-bio pingouin scikit-learn matplotlib pandas numpy scipy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gzip\n",
        "import scipy.io\n",
        "from scipy.sparse import csr_matrix\n",
        "import scanpy as sc\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from skbio.stats.distance import mantel, DistanceMatrix\n",
        "from scipy.stats import spearmanr\n",
        "import pingouin as pg\n",
        "import matplotlib.pyplot as plt\n",
        "import os, json\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCqcgurYjOxU",
        "outputId": "e21f0fa1-baa4-4e22-a0c4-5cc1049103db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18A0K7W7izGv",
        "outputId": "f47d26d6-f77d-4393-b8c6-89f2bd89b171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== COMBINATORIAL PERTURBATION ANALYSIS ===\n",
            "\n",
            "Loading GSE133344 dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/anndata/_core/anndata.py:1793: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
            "  utils.warn_names_duplicates(\"var\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset: 111445 cells x 33694 genes\n",
            "Perturbation guides: 290\n",
            "Perturbation type distribution:\n",
            "perturbation_type\n",
            "single     57831\n",
            "dual       41759\n",
            "control    11855\n",
            "Name: count, dtype: int64\n",
            "Loading GeneRAIN embeddings from /content/drive/MyDrive/dataset-gene-embed/GeneRAIN-vec.200d.txt...\n",
            "Metadata: 31769 200\n",
            "Loaded GeneRAIN: 31769 genes, 200 dimensions\n",
            "Created pseudobulk for 290 combinations\n",
            "Computed effects for 290 combinations\n",
            "Created combined embeddings: 284 combinations, 200 dimensions\n",
            "Common perturbation combinations: 284\n",
            "\n",
            "=== COMBINATORIAL ANALYSIS RESULTS ===\n",
            "Combinations analyzed: 284\n",
            "Correlation: ρ = 0.081, p = 1.76e-59\n",
            "Mantel test: r = 0.081, p = 1.00e-03\n",
            "Precision@10: 0.328\n",
            "Random baseline: 0.035\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# File paths\n",
        "base_path = \"/content/drive/MyDrive/dataset-gene-embed/\"\n",
        "files = {\n",
        "    'barcodes': base_path + \"GSE133344_filtered_barcodes.tsv.gz\",\n",
        "    'cell_identities': base_path + \"GSE133344_filtered_cell_identities.csv.gz\",\n",
        "    'matrix': base_path + \"GSE133344_filtered_matrix.mtx\",\n",
        "    'genes': base_path + \"GSE133344_filtered_genes.tsv\",\n",
        "    'generain': base_path + \"GeneRAIN-vec.200d.txt\"\n",
        "}\n",
        "\n",
        "# Load GSE133344 data\n",
        "def load_gse133344_data():\n",
        "    \"\"\"Load GSE133344 dataset and create AnnData object\"\"\"\n",
        "    print(\"Loading GSE133344 dataset...\")\n",
        "\n",
        "    # Load genes\n",
        "    genes = pd.read_csv(files['genes'], sep='\\t', header=None, names=['ensembl_id', 'gene_symbol'])\n",
        "\n",
        "    # Load barcodes\n",
        "    with gzip.open(files['barcodes'], 'rt') as f:\n",
        "        barcodes = [line.strip() for line in f]\n",
        "\n",
        "    # Load cell identities\n",
        "    cell_identities = pd.read_csv(files['cell_identities'], compression='gzip')\n",
        "\n",
        "    # Load expression matrix\n",
        "    matrix = scipy.io.mmread(files['matrix']).T.tocsr()  # Transpose to cells x genes\n",
        "\n",
        "    # Create AnnData object\n",
        "    adata = sc.AnnData(X=matrix)\n",
        "    adata.obs_names = barcodes\n",
        "    adata.var_names = genes['gene_symbol'].values\n",
        "    adata.var['ensembl_id'] = genes['ensembl_id'].values\n",
        "\n",
        "    # Add cell annotations (align by barcode)\n",
        "    cell_identities = cell_identities.set_index('cell_barcode')\n",
        "    # Only keep cells that are in both datasets\n",
        "    common_cells = list(set(adata.obs_names) & set(cell_identities.index))\n",
        "    adata = adata[common_cells].copy()\n",
        "    adata.obs = adata.obs.join(cell_identities.loc[common_cells])\n",
        "\n",
        "    print(f\"Loaded dataset: {adata.n_obs} cells x {adata.n_vars} genes\")\n",
        "    print(f\"Perturbation guides: {adata.obs['guide_identity'].nunique()}\")\n",
        "\n",
        "    return adata\n",
        "\n",
        "# Parse combinatorial targets\n",
        "def parse_combinatorial_targets(guide_identity):\n",
        "    \"\"\"\n",
        "    Parse guide identities to extract target genes.\n",
        "    Format: GENE1_GENE2__GENE1_GENE2 or GENE1_NegCtrl0__GENE1_NegCtrl0\n",
        "    \"\"\"\n",
        "    if pd.isna(guide_identity):\n",
        "        return []\n",
        "\n",
        "    # Extract the first part before '__'\n",
        "    target_part = guide_identity.split('__')[0]\n",
        "\n",
        "    # Split by underscore to get individual components\n",
        "    components = target_part.split('_')\n",
        "\n",
        "    # Filter out control components and extract real gene targets\n",
        "    targets = []\n",
        "    for comp in components:\n",
        "        if 'NegCtrl' not in comp and len(comp) > 0:\n",
        "            targets.append(comp)\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    unique_targets = []\n",
        "    for t in targets:\n",
        "        if t not in unique_targets:\n",
        "            unique_targets.append(t)\n",
        "\n",
        "    return unique_targets\n",
        "\n",
        "def categorize_perturbations(adata):\n",
        "    \"\"\"Categorize perturbations into control, single-gene, and combinatorial\"\"\"\n",
        "    adata.obs['parsed_targets'] = adata.obs['guide_identity'].apply(parse_combinatorial_targets)\n",
        "    adata.obs['n_targets'] = adata.obs['parsed_targets'].apply(len)\n",
        "\n",
        "    # Create perturbation categories\n",
        "    adata.obs['perturbation_type'] = 'unknown'\n",
        "    adata.obs.loc[adata.obs['n_targets'] == 0, 'perturbation_type'] = 'control'\n",
        "    adata.obs.loc[adata.obs['n_targets'] == 1, 'perturbation_type'] = 'single'\n",
        "    adata.obs.loc[adata.obs['n_targets'] == 2, 'perturbation_type'] = 'dual'\n",
        "    adata.obs.loc[adata.obs['n_targets'] > 2, 'perturbation_type'] = 'multi'\n",
        "\n",
        "    print(\"Perturbation type distribution:\")\n",
        "    print(adata.obs['perturbation_type'].value_counts())\n",
        "\n",
        "    return adata\n",
        "\n",
        "# Load GeneRAIN embeddings\n",
        "def load_generain_embeddings(file_path):\n",
        "    \"\"\"Load GeneRAIN embeddings\"\"\"\n",
        "    print(f\"Loading GeneRAIN embeddings from {file_path}...\")\n",
        "\n",
        "    genes = []\n",
        "    embeddings = []\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "        # Skip metadata line\n",
        "        first_line = f.readline().strip()\n",
        "        print(f\"Metadata: {first_line}\")\n",
        "\n",
        "        for line_num, line in enumerate(f, 2):\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "\n",
        "            gene_name = parts[0]\n",
        "            embedding_values = [float(x) for x in parts[1:]]\n",
        "            genes.append(gene_name)\n",
        "            embeddings.append(embedding_values)\n",
        "\n",
        "    df = pd.DataFrame(embeddings, index=genes)\n",
        "    df.columns = [f\"generain_{i}\" for i in range(df.shape[1])]\n",
        "\n",
        "    print(f\"Loaded GeneRAIN: {df.shape[0]} genes, {df.shape[1]} dimensions\")\n",
        "    return df\n",
        "\n",
        "# Create combined embeddings for gene combinations\n",
        "def create_combined_embeddings(target_combinations, generain_embeddings, method='average'):\n",
        "    \"\"\"\n",
        "    Create embeddings for gene combinations.\n",
        "    Methods: 'average', 'concatenate', 'element_wise_product'\n",
        "    \"\"\"\n",
        "    combined_embeddings = {}\n",
        "\n",
        "    for combo_name, gene_list in target_combinations.items():\n",
        "        if len(gene_list) == 0:\n",
        "            # Control - use zero vector\n",
        "            combined_embeddings[combo_name] = np.zeros(generain_embeddings.shape[1])\n",
        "        elif len(gene_list) == 1:\n",
        "            # Single gene\n",
        "            gene = gene_list[0]\n",
        "            if gene in generain_embeddings.index:\n",
        "                combined_embeddings[combo_name] = generain_embeddings.loc[gene].values\n",
        "            else:\n",
        "                # Gene not found, skip\n",
        "                continue\n",
        "        else:\n",
        "            # Multiple genes - combine embeddings\n",
        "            available_genes = [g for g in gene_list if g in generain_embeddings.index]\n",
        "\n",
        "            if len(available_genes) == 0:\n",
        "                continue  # No genes found in embeddings\n",
        "\n",
        "            gene_embeds = [generain_embeddings.loc[g].values for g in available_genes]\n",
        "\n",
        "            if method == 'average':\n",
        "                combined_embeddings[combo_name] = np.mean(gene_embeds, axis=0)\n",
        "            elif method == 'concatenate':\n",
        "                # Pad shorter lists to same length for concatenation\n",
        "                max_genes = 2  # Assume max 2 genes per combination\n",
        "                while len(gene_embeds) < max_genes:\n",
        "                    gene_embeds.append(np.zeros(generain_embeddings.shape[1]))\n",
        "                combined_embeddings[combo_name] = np.concatenate(gene_embeds[:max_genes])\n",
        "            elif method == 'element_wise_product':\n",
        "                combined_embeddings[combo_name] = np.prod(gene_embeds, axis=0)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    combo_embedding_df = pd.DataFrame.from_dict(combined_embeddings, orient='index')\n",
        "    print(f\"Created combined embeddings: {combo_embedding_df.shape[0]} combinations, {combo_embedding_df.shape[1]} dimensions\")\n",
        "\n",
        "    return combo_embedding_df\n",
        "\n",
        "# Pseudobulk aggregation for combinations\n",
        "def make_combinatorial_pseudobulk(adata, min_cells=50):\n",
        "    \"\"\"Create pseudobulk for each perturbation combination\"\"\"\n",
        "    # Group by guide identity\n",
        "    groups = adata.obs.groupby('guide_identity').indices\n",
        "\n",
        "    pseudobulk_data = []\n",
        "    meta_data = []\n",
        "\n",
        "    for guide, cell_idx in groups.items():\n",
        "        if len(cell_idx) < min_cells:\n",
        "            continue\n",
        "\n",
        "        # Sum counts for this combination\n",
        "        summed_counts = np.array(adata.X[cell_idx].sum(axis=0)).ravel()\n",
        "\n",
        "        pseudobulk_data.append(summed_counts)\n",
        "        meta_data.append({\n",
        "            'guide_identity': guide,\n",
        "            'n_cells': len(cell_idx),\n",
        "            'targets': adata.obs.loc[adata.obs_names[cell_idx[0]], 'parsed_targets'],\n",
        "            'perturbation_type': adata.obs.loc[adata.obs_names[cell_idx[0]], 'perturbation_type']\n",
        "        })\n",
        "\n",
        "    pseudobulk_df = pd.DataFrame(pseudobulk_data, columns=adata.var_names)\n",
        "    pseudobulk_df.index = [m['guide_identity'] for m in meta_data]\n",
        "    meta_df = pd.DataFrame(meta_data)\n",
        "\n",
        "    print(f\"Created pseudobulk for {len(pseudobulk_df)} combinations\")\n",
        "    return pseudobulk_df, meta_df\n",
        "\n",
        "def compute_combinatorial_effects(pseudobulk_df, meta_df):\n",
        "    \"\"\"Compute perturbation effects relative to controls\"\"\"\n",
        "    # Normalize to CPM and log-transform\n",
        "    lib_sizes = pseudobulk_df.sum(axis=1)\n",
        "    cpm = pseudobulk_df.div(lib_sizes, axis=0) * 1e6\n",
        "    logcpm = np.log1p(cpm)\n",
        "\n",
        "    # Identify controls\n",
        "    control_guides = meta_df[meta_df['perturbation_type'] == 'control']['guide_identity'].tolist()\n",
        "\n",
        "    if len(control_guides) == 0:\n",
        "        raise ValueError(\"No control perturbations found\")\n",
        "\n",
        "    # Compute mean control profile\n",
        "    control_profiles = logcpm.loc[control_guides]\n",
        "    mean_control = control_profiles.mean(axis=0)\n",
        "\n",
        "    # Compute effects (perturbation - control)\n",
        "    effects = logcpm.subtract(mean_control, axis=1)\n",
        "\n",
        "    # Z-score standardize each gene\n",
        "    effects_zscore = effects.subtract(effects.mean(axis=0), axis=1)\n",
        "    effects_zscore = effects_zscore.divide(effects_zscore.std(axis=0), axis=1)\n",
        "\n",
        "    print(f\"Computed effects for {len(effects_zscore)} combinations\")\n",
        "    return effects_zscore\n",
        "\n",
        "# Distance calculations and statistics (same as before)\n",
        "def pairwise_cosine(M):\n",
        "    if np.isnan(M).any():\n",
        "        M = np.nan_to_num(M, nan=0.0)\n",
        "    return pairwise_distances(M, metric='cosine')\n",
        "\n",
        "def mantel_and_spearman(D1, D2, n_perms=999):\n",
        "    if np.isnan(D1).any() or np.isnan(D2).any():\n",
        "        D1 = np.nan_to_num(D1, nan=1.0)\n",
        "        D2 = np.nan_to_num(D2, nan=1.0)\n",
        "\n",
        "    iu = np.triu_indices_from(D1, k=1)\n",
        "    rho, p = spearmanr(D1[iu], D2[iu])\n",
        "\n",
        "    if n_perms > 0:\n",
        "        try:\n",
        "            ids = [f\"X{i}\" for i in range(D1.shape[0])]\n",
        "            m1, m2 = DistanceMatrix(D1, ids=ids), DistanceMatrix(D2, ids=ids)\n",
        "            r_m, p_m, _ = mantel(m1, m2, method='spearman', permutations=n_perms)\n",
        "            return rho, p, r_m, p_m\n",
        "        except:\n",
        "            return rho, p, rho, p\n",
        "    return rho, p, rho, p\n",
        "\n",
        "def retrieval_metrics(Dg, Dp, ks=(5,10,20)):\n",
        "    n = Dg.shape[0]\n",
        "    Dg2, Dp2 = Dg.copy(), Dp.copy()\n",
        "    np.fill_diagonal(Dg2, np.inf)\n",
        "    np.fill_diagonal(Dp2, np.inf)\n",
        "\n",
        "    precisions = {k: [] for k in ks}\n",
        "    for i in range(n):\n",
        "        rank_g = np.argsort(Dg2[i])\n",
        "        rank_p = np.argsort(Dp2[i])\n",
        "        for k in ks:\n",
        "            precisions[k].append(len(set(rank_g[:k]) & set(rank_p[:k]))/k)\n",
        "\n",
        "    return {k: float(np.mean(v)) for k, v in precisions.items()}\n",
        "\n",
        "# Plotting functions\n",
        "def plot_distance_scatter(Dg, Dp, title, out=\"figs/combinatorial_scatter.png\"):\n",
        "    os.makedirs(\"figs\", exist_ok=True)\n",
        "    iu = np.triu_indices_from(Dg, k=1)\n",
        "    x, y = Dg[iu], Dp[iu]\n",
        "\n",
        "    rho, p = spearmanr(x, y)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.scatter(x, y, s=8, alpha=0.6)\n",
        "    plt.title(f\"{title}\\nSpearman ρ={rho:.3f}, p={p:.2e}\")\n",
        "    plt.xlabel(\"Combined Embedding Distance\")\n",
        "    plt.ylabel(\"Combinatorial Perturbation Distance\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out, dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "# Main analysis function\n",
        "def run_combinatorial_analysis():\n",
        "    \"\"\"Run the full combinatorial perturbation analysis\"\"\"\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "    print(\"=== COMBINATORIAL PERTURBATION ANALYSIS ===\\n\")\n",
        "\n",
        "    # Load data\n",
        "    adata = load_gse133344_data()\n",
        "    adata = categorize_perturbations(adata)\n",
        "\n",
        "    # Load GeneRAIN embeddings\n",
        "    generain_emb = load_generain_embeddings(files['generain'])\n",
        "\n",
        "    # Create pseudobulk\n",
        "    pseudobulk_df, meta_df = make_combinatorial_pseudobulk(adata, min_cells=50)\n",
        "\n",
        "    # Compute effects\n",
        "    effects_df = compute_combinatorial_effects(pseudobulk_df, meta_df)\n",
        "\n",
        "    # Create target combination mapping\n",
        "    target_combinations = {}\n",
        "    for idx, row in meta_df.iterrows():\n",
        "        guide = row['guide_identity']\n",
        "        targets = row['targets']\n",
        "        target_combinations[guide] = targets\n",
        "\n",
        "    # Create combined embeddings for the combinations we have data for\n",
        "    available_guides = effects_df.index.tolist()\n",
        "    available_combinations = {g: target_combinations[g] for g in available_guides if g in target_combinations}\n",
        "\n",
        "    combined_embeddings = create_combined_embeddings(available_combinations, generain_emb, method='average')\n",
        "\n",
        "    # Align datasets\n",
        "    common_guides = list(set(effects_df.index) & set(combined_embeddings.index))\n",
        "    print(f\"Common perturbation combinations: {len(common_guides)}\")\n",
        "\n",
        "    if len(common_guides) < 10:\n",
        "        print(\"ERROR: Too few combinations for analysis\")\n",
        "        return None\n",
        "\n",
        "    effects_aligned = effects_df.loc[common_guides]\n",
        "    embeddings_aligned = combined_embeddings.loc[common_guides]\n",
        "\n",
        "    # Compute distance matrices\n",
        "    Dp = pairwise_cosine(effects_aligned.values)\n",
        "    Dg = pairwise_cosine(embeddings_aligned.values)\n",
        "\n",
        "    # Statistical analysis\n",
        "    rho, p, r_mantel, p_mantel = mantel_and_spearman(Dg, Dp, n_perms=999)\n",
        "    retrieval_results = retrieval_metrics(Dg, Dp)\n",
        "\n",
        "    # Results\n",
        "    results = {\n",
        "        'analysis_type': 'combinatorial',\n",
        "        'n_combinations': len(common_guides),\n",
        "        'embedding_method': 'average',\n",
        "        'spearman_rho': float(rho),\n",
        "        'spearman_p': float(p),\n",
        "        'mantel_r': float(r_mantel),\n",
        "        'mantel_p': float(p_mantel),\n",
        "        'precision_at': retrieval_results\n",
        "    }\n",
        "\n",
        "    # Save results\n",
        "    with open(\"results/combinatorial_metrics.json\", \"w\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    # Plot\n",
        "    plot_distance_scatter(Dg, Dp, \"Combinatorial Perturbation Analysis\")\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n=== COMBINATORIAL ANALYSIS RESULTS ===\")\n",
        "    print(f\"Combinations analyzed: {len(common_guides)}\")\n",
        "    print(f\"Correlation: ρ = {rho:.3f}, p = {p:.2e}\")\n",
        "    print(f\"Mantel test: r = {r_mantel:.3f}, p = {p_mantel:.2e}\")\n",
        "    print(f\"Precision@10: {retrieval_results[10]:.3f}\")\n",
        "    print(f\"Random baseline: {10/(len(common_guides)-1):.3f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run analysis\n",
        "results = run_combinatorial_analysis()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install scanpy anndata scikit-bio pingouin scikit-learn matplotlib pandas numpy scipy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gzip\n",
        "import scipy.io\n",
        "from scipy.sparse import csr_matrix\n",
        "import scanpy as sc\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from skbio.stats.distance import mantel, DistanceMatrix\n",
        "from scipy.stats import spearmanr\n",
        "import pingouin as pg\n",
        "import matplotlib.pyplot as plt\n",
        "import os, json\n",
        "from scipy import sparse\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# File paths\n",
        "base_path = \"/content/drive/MyDrive/dataset-gene-embed/\"\n",
        "files = {\n",
        "    'barcodes': base_path + \"GSE133344_filtered_barcodes.tsv.gz\",\n",
        "    'cell_identities': base_path + \"GSE133344_filtered_cell_identities.csv.gz\",\n",
        "    'matrix': base_path + \"GSE133344_filtered_matrix.mtx\",\n",
        "    'genes': base_path + \"GSE133344_filtered_genes.tsv\",\n",
        "    'generain': base_path + \"GeneRAIN-vec.200d.txt\"\n",
        "}\n",
        "\n",
        "# Load GSE133344 data\n",
        "def load_gse133344_data():\n",
        "    \"\"\"Load GSE133344 dataset and create AnnData object\"\"\"\n",
        "    print(\"Loading GSE133344 dataset...\")\n",
        "\n",
        "    # Load genes\n",
        "    genes = pd.read_csv(files['genes'], sep='\\t', header=None, names=['ensembl_id', 'gene_symbol'])\n",
        "\n",
        "    # Load barcodes\n",
        "    with gzip.open(files['barcodes'], 'rt') as f:\n",
        "        barcodes = [line.strip() for line in f]\n",
        "\n",
        "    # Load cell identities\n",
        "    cell_identities = pd.read_csv(files['cell_identities'], compression='gzip')\n",
        "\n",
        "    # Load expression matrix\n",
        "    matrix = scipy.io.mmread(files['matrix']).T.tocsr()  # Transpose to cells x genes\n",
        "\n",
        "    # Create AnnData object\n",
        "    adata = sc.AnnData(X=matrix)\n",
        "    adata.obs_names = barcodes\n",
        "    adata.var_names = genes['gene_symbol'].values\n",
        "    adata.var['ensembl_id'] = genes['ensembl_id'].values\n",
        "    adata.var_names_make_unique()  # Fix duplicate names\n",
        "\n",
        "    # Add cell annotations (align by barcode)\n",
        "    cell_identities = cell_identities.set_index('cell_barcode')\n",
        "    # Only keep cells that are in both datasets\n",
        "    common_cells = list(set(adata.obs_names) & set(cell_identities.index))\n",
        "    adata = adata[common_cells].copy()\n",
        "    adata.obs = adata.obs.join(cell_identities.loc[common_cells])\n",
        "\n",
        "    print(f\"Loaded dataset: {adata.n_obs} cells x {adata.n_vars} genes\")\n",
        "    print(f\"Perturbation guides: {adata.obs['guide_identity'].nunique()}\")\n",
        "    print(f\"Gemgroups: {sorted(adata.obs['gemgroup'].unique())}\")\n",
        "\n",
        "    return adata\n",
        "\n",
        "# Parse combinatorial targets\n",
        "def parse_combinatorial_targets(guide_identity):\n",
        "    \"\"\"Parse guide identities to extract target genes\"\"\"\n",
        "    if pd.isna(guide_identity):\n",
        "        return []\n",
        "\n",
        "    # Extract the first part before '__'\n",
        "    target_part = guide_identity.split('__')[0]\n",
        "    components = target_part.split('_')\n",
        "\n",
        "    # Filter out control components and extract real gene targets\n",
        "    targets = []\n",
        "    for comp in components:\n",
        "        if 'NegCtrl' not in comp and len(comp) > 0:\n",
        "            targets.append(comp)\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    unique_targets = []\n",
        "    for t in targets:\n",
        "        if t not in unique_targets:\n",
        "            unique_targets.append(t)\n",
        "\n",
        "    return unique_targets\n",
        "\n",
        "def categorize_perturbations(adata):\n",
        "    \"\"\"Categorize perturbations into control, single-gene, and combinatorial\"\"\"\n",
        "    adata.obs['parsed_targets'] = adata.obs['guide_identity'].apply(parse_combinatorial_targets)\n",
        "    adata.obs['n_targets'] = adata.obs['parsed_targets'].apply(len)\n",
        "\n",
        "    # Create perturbation categories\n",
        "    adata.obs['perturbation_type'] = 'unknown'\n",
        "    adata.obs.loc[adata.obs['n_targets'] == 0, 'perturbation_type'] = 'control'\n",
        "    adata.obs.loc[adata.obs['n_targets'] == 1, 'perturbation_type'] = 'single'\n",
        "    adata.obs.loc[adata.obs['n_targets'] == 2, 'perturbation_type'] = 'dual'\n",
        "    adata.obs.loc[adata.obs['n_targets'] > 2, 'perturbation_type'] = 'multi'\n",
        "\n",
        "    print(\"Perturbation type distribution:\")\n",
        "    print(adata.obs['perturbation_type'].value_counts())\n",
        "\n",
        "    return adata\n",
        "\n",
        "# Load GeneRAIN embeddings\n",
        "def load_generain_embeddings(file_path):\n",
        "    \"\"\"Load GeneRAIN embeddings\"\"\"\n",
        "    print(f\"Loading GeneRAIN embeddings from {file_path}...\")\n",
        "\n",
        "    genes = []\n",
        "    embeddings = []\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "        # Skip metadata line\n",
        "        first_line = f.readline().strip()\n",
        "        print(f\"Metadata: {first_line}\")\n",
        "\n",
        "        for line_num, line in enumerate(f, 2):\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "\n",
        "            gene_name = parts[0]\n",
        "            embedding_values = [float(x) for x in parts[1:]]\n",
        "            genes.append(gene_name)\n",
        "            embeddings.append(embedding_values)\n",
        "\n",
        "    df = pd.DataFrame(embeddings, index=genes)\n",
        "    df.columns = [f\"generain_{i}\" for i in range(df.shape[1])]\n",
        "\n",
        "    print(f\"Loaded GeneRAIN: {df.shape[0]} genes, {df.shape[1]} dimensions\")\n",
        "    return df\n",
        "\n",
        "# IMPROVED: Gemgroup-matched pseudobulk\n",
        "def make_gemgroup_matched_combinatorial_pseudobulk(adata, min_cells=20):\n",
        "    \"\"\"Create pseudobulk with gemgroup matching for combinatorial analysis\"\"\"\n",
        "    print(\"Creating gemgroup-matched pseudobulks for combinatorial analysis...\")\n",
        "\n",
        "    # Group by guide_identity AND gemgroup\n",
        "    groups = adata.obs.groupby(['guide_identity', 'gemgroup']).indices\n",
        "\n",
        "    pseudobulk_data = []\n",
        "    meta_data = []\n",
        "\n",
        "    X = adata.X\n",
        "    is_sparse = sparse.issparse(X)\n",
        "\n",
        "    for (guide, gemgroup), cell_idx in groups.items():\n",
        "        if len(cell_idx) < min_cells:\n",
        "            continue\n",
        "\n",
        "        # Sum counts for this guide-gemgroup combination\n",
        "        if is_sparse:\n",
        "            summed_counts = np.array(X[cell_idx].sum(axis=0)).ravel()\n",
        "        else:\n",
        "            summed_counts = X[cell_idx].sum(axis=0)\n",
        "\n",
        "        pseudobulk_data.append(summed_counts)\n",
        "        meta_data.append({\n",
        "            'guide_identity': guide,\n",
        "            'gemgroup': gemgroup,\n",
        "            'n_cells': len(cell_idx),\n",
        "            'targets': adata.obs.loc[adata.obs_names[cell_idx[0]], 'parsed_targets'],\n",
        "            'perturbation_type': adata.obs.loc[adata.obs_names[cell_idx[0]], 'perturbation_type']\n",
        "        })\n",
        "\n",
        "    pseudobulk_df = pd.DataFrame(pseudobulk_data, columns=adata.var_names)\n",
        "    pseudobulk_df.index = pd.MultiIndex.from_frame(pd.DataFrame(meta_data)[['guide_identity', 'gemgroup']])\n",
        "    meta_df = pd.DataFrame(meta_data)\n",
        "\n",
        "    print(f\"Created pseudobulk for {len(pseudobulk_df)} guide-gemgroup combinations\")\n",
        "    print(f\"Gemgroups with data: {meta_df['gemgroup'].nunique()}\")\n",
        "    print(f\"Control combinations: {(meta_df['perturbation_type'] == 'control').sum()}\")\n",
        "\n",
        "    return pseudobulk_df, meta_df\n",
        "\n",
        "# IMPROVED: Gemgroup-matched effect computation\n",
        "def compute_gemgroup_matched_combinatorial_effects(pseudobulk_df, meta_df):\n",
        "    \"\"\"Compute effects with gemgroup-matched controls for combinatorial analysis\"\"\"\n",
        "    print(\"Computing gemgroup-matched effects for combinatorial analysis...\")\n",
        "\n",
        "    # Normalize to CPM and log-transform\n",
        "    lib_sizes = pseudobulk_df.sum(axis=1)\n",
        "    cpm = pseudobulk_df.div(lib_sizes, axis=0) * 1e6\n",
        "    logcpm = np.log1p(cpm)\n",
        "\n",
        "    # Identify control guides\n",
        "    control_guides = set(meta_df[meta_df['perturbation_type'] == 'control']['guide_identity'])\n",
        "\n",
        "    if len(control_guides) == 0:\n",
        "        raise ValueError(\"No control perturbations found\")\n",
        "\n",
        "    print(f\"Found {len(control_guides)} control guides\")\n",
        "\n",
        "    # Get unique guides (excluding controls for effects)\n",
        "    all_guides = sorted(set(meta_df['guide_identity']))\n",
        "    target_guides = [g for g in all_guides if g not in control_guides]\n",
        "\n",
        "    print(f\"Computing effects for {len(target_guides)} target guides\")\n",
        "\n",
        "    effects = {}\n",
        "\n",
        "    for guide in target_guides:\n",
        "        try:\n",
        "            guide_profiles = logcpm.loc[guide]\n",
        "            if isinstance(guide_profiles, pd.Series):\n",
        "                guide_profiles = pd.DataFrame([guide_profiles.values],\n",
        "                                            index=[guide_profiles.name],\n",
        "                                            columns=logcpm.columns)\n",
        "\n",
        "            guide_effects = []\n",
        "\n",
        "            # For each gemgroup where this guide appears\n",
        "            for gemgroup in guide_profiles.index:\n",
        "                # Find control profiles in the same gemgroup\n",
        "                control_profiles_in_gemgroup = []\n",
        "                for control_guide in control_guides:\n",
        "                    try:\n",
        "                        ctrl_profile = logcpm.loc[(control_guide, gemgroup)]\n",
        "                        control_profiles_in_gemgroup.append(ctrl_profile)\n",
        "                    except KeyError:\n",
        "                        continue\n",
        "\n",
        "                if control_profiles_in_gemgroup:\n",
        "                    # Average control profiles in this gemgroup\n",
        "                    if len(control_profiles_in_gemgroup) == 1:\n",
        "                        control_mean = control_profiles_in_gemgroup[0]\n",
        "                    else:\n",
        "                        control_mean = pd.concat(control_profiles_in_gemgroup, axis=1).mean(axis=1)\n",
        "\n",
        "                    # Compute effect: target - matched control\n",
        "                    effect = guide_profiles.loc[gemgroup] - control_mean\n",
        "                    guide_effects.append(effect)\n",
        "                else:\n",
        "                    print(f\"Warning: No control found for guide {guide} in gemgroup {gemgroup}\")\n",
        "\n",
        "            if guide_effects:\n",
        "                # Average effects across gemgroups\n",
        "                if len(guide_effects) == 1:\n",
        "                    mean_effect = guide_effects[0]\n",
        "                else:\n",
        "                    mean_effect = pd.concat(guide_effects, axis=1).mean(axis=1)\n",
        "                effects[guide] = mean_effect.values\n",
        "\n",
        "        except KeyError:\n",
        "            print(f\"Warning: Could not process guide {guide}\")\n",
        "            continue\n",
        "\n",
        "    if len(effects) == 0:\n",
        "        raise ValueError(\"No effects computed\")\n",
        "\n",
        "    # Create effects DataFrame\n",
        "    effects_df = pd.DataFrame.from_dict(effects, orient='index', columns=logcpm.columns)\n",
        "\n",
        "    # Z-score standardize each gene (with improved handling)\n",
        "    effects_mean = effects_df.mean(axis=0)\n",
        "    effects_std = effects_df.std(axis=0, ddof=0)\n",
        "    effects_std = effects_std.replace(0, 1.0)  # Avoid division by zero\n",
        "\n",
        "    effects_zscore = effects_df.subtract(effects_mean, axis=1).divide(effects_std, axis=1)\n",
        "    effects_zscore = effects_zscore.replace([np.inf, -np.inf], 0).fillna(0)\n",
        "\n",
        "    print(f\"Computed effects for {len(effects_zscore)} guides\")\n",
        "    print(f\"Effect range: [{effects_zscore.values.min():.3f}, {effects_zscore.values.max():.3f}]\")\n",
        "\n",
        "    return effects_zscore\n",
        "\n",
        "# IMPROVED: Multiple combination operators\n",
        "def create_combined_embeddings_multi_method(target_combinations, generain_embeddings, method='average'):\n",
        "    \"\"\"Create embeddings with different combination methods\"\"\"\n",
        "    combined_embeddings = {}\n",
        "\n",
        "    for combo_name, gene_list in target_combinations.items():\n",
        "        if len(gene_list) == 0:\n",
        "            # Control - use zero vector\n",
        "            combined_embeddings[combo_name] = np.zeros(generain_embeddings.shape[1])\n",
        "        elif len(gene_list) == 1:\n",
        "            # Single gene\n",
        "            gene = gene_list[0]\n",
        "            if gene in generain_embeddings.index:\n",
        "                combined_embeddings[combo_name] = generain_embeddings.loc[gene].values\n",
        "            else:\n",
        "                continue  # Skip if gene not found\n",
        "        else:\n",
        "            # Multiple genes - combine embeddings\n",
        "            available_genes = [g for g in gene_list if g in generain_embeddings.index]\n",
        "\n",
        "            if len(available_genes) == 0:\n",
        "                continue  # Skip if no genes found\n",
        "\n",
        "            gene_embeds = np.stack([generain_embeddings.loc[g].values for g in available_genes])\n",
        "\n",
        "            if method == 'average':\n",
        "                combined_embeddings[combo_name] = np.mean(gene_embeds, axis=0)\n",
        "            elif method == 'sum':\n",
        "                combined_embeddings[combo_name] = np.sum(gene_embeds, axis=0)\n",
        "            elif method == 'hadamard':\n",
        "                combined_embeddings[combo_name] = np.prod(gene_embeds, axis=0)\n",
        "            elif method == 'concatenate':\n",
        "                # Pad to consistent length (assume max 2 genes)\n",
        "                if len(gene_embeds) == 1:\n",
        "                    padded = np.concatenate([gene_embeds[0], np.zeros_like(gene_embeds[0])])\n",
        "                else:\n",
        "                    padded = np.concatenate(gene_embeds[:2])  # Take first 2\n",
        "                combined_embeddings[combo_name] = padded\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    combo_embedding_df = pd.DataFrame.from_dict(combined_embeddings, orient='index')\n",
        "    print(f\"Created {method} embeddings: {combo_embedding_df.shape[0]} combinations, {combo_embedding_df.shape[1]} dimensions\")\n",
        "\n",
        "    return combo_embedding_df\n",
        "\n",
        "def test_combination_operators(target_combinations, generain_embeddings):\n",
        "    \"\"\"Test different ways to combine gene embeddings\"\"\"\n",
        "    print(\"Testing different combination operators...\")\n",
        "\n",
        "    combination_methods = ['average', 'sum', 'hadamard']\n",
        "    results = {}\n",
        "\n",
        "    for method in combination_methods:\n",
        "        print(f\"  Creating {method} combinations...\")\n",
        "        combined_embeddings = create_combined_embeddings_multi_method(\n",
        "            target_combinations, generain_embeddings, method=method)\n",
        "        results[method] = combined_embeddings\n",
        "\n",
        "    return results\n",
        "\n",
        "# Distance calculations with error handling\n",
        "def pairwise_cosine(M):\n",
        "    \"\"\"Compute cosine distances with comprehensive error handling\"\"\"\n",
        "    # Handle NaN and infinity values\n",
        "    M = np.nan_to_num(M, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "\n",
        "    # Check for zero-norm rows\n",
        "    row_norms = np.linalg.norm(M, axis=1)\n",
        "    zero_norm_mask = row_norms == 0\n",
        "\n",
        "    if zero_norm_mask.any():\n",
        "        print(f\"Warning: {zero_norm_mask.sum()} zero-norm rows detected, adding small noise\")\n",
        "        M[zero_norm_mask] += np.random.normal(0, 1e-6, size=(zero_norm_mask.sum(), M.shape[1]))\n",
        "\n",
        "    try:\n",
        "        distances = pairwise_distances(M, metric='cosine')\n",
        "        np.fill_diagonal(distances, 0.0)\n",
        "        distances = np.nan_to_num(distances, nan=1.0, posinf=1.0, neginf=0.0)\n",
        "        return distances\n",
        "    except Exception as e:\n",
        "        print(f\"Distance computation failed: {e}\")\n",
        "        n = M.shape[0]\n",
        "        return np.eye(n)\n",
        "\n",
        "# Statistical analysis functions\n",
        "def mantel_and_spearman(D1, D2, n_perms=999):\n",
        "    \"\"\"Compute Spearman correlation and Mantel test\"\"\"\n",
        "    if np.isnan(D1).any() or np.isnan(D2).any():\n",
        "        D1 = np.nan_to_num(D1, nan=1.0)\n",
        "        D2 = np.nan_to_num(D2, nan=1.0)\n",
        "\n",
        "    iu = np.triu_indices_from(D1, k=1)\n",
        "    dist1, dist2 = D1[iu], D2[iu]\n",
        "\n",
        "    # Remove any remaining invalid values\n",
        "    valid_mask = ~(np.isnan(dist1) | np.isnan(dist2) | np.isinf(dist1) | np.isinf(dist2))\n",
        "    if not valid_mask.all():\n",
        "        print(f\"Removing {(~valid_mask).sum()} invalid distance pairs\")\n",
        "        dist1, dist2 = dist1[valid_mask], dist2[valid_mask]\n",
        "\n",
        "    if len(dist1) == 0:\n",
        "        return 0.0, 1.0, 0.0, 1.0\n",
        "\n",
        "    rho, p = spearmanr(dist1, dist2)\n",
        "\n",
        "    if n_perms > 0:\n",
        "        try:\n",
        "            ids = [f\"X{i}\" for i in range(D1.shape[0])]\n",
        "            m1, m2 = DistanceMatrix(D1, ids=ids), DistanceMatrix(D2, ids=ids)\n",
        "            r_m, p_m, _ = mantel(m1, m2, method='spearman', permutations=n_perms)\n",
        "            return rho, p, r_m, p_m\n",
        "        except Exception as e:\n",
        "            print(f\"Mantel test failed: {e}\")\n",
        "            return rho, p, rho, p\n",
        "    else:\n",
        "        return rho, p, rho, p\n",
        "\n",
        "def retrieval_metrics(Dg, Dp, ks=(5,10,20)):\n",
        "    \"\"\"Compute retrieval metrics\"\"\"\n",
        "    n = Dg.shape[0]\n",
        "    Dg2, Dp2 = Dg.copy(), Dp.copy()\n",
        "    np.fill_diagonal(Dg2, np.inf)\n",
        "    np.fill_diagonal(Dp2, np.inf)\n",
        "\n",
        "    precisions = {k: [] for k in ks}\n",
        "    aps = []\n",
        "\n",
        "    for i in range(n):\n",
        "        rank_g = np.argsort(Dg2[i])\n",
        "        rank_p = np.argsort(Dp2[i])\n",
        "\n",
        "        for k in ks:\n",
        "            precisions[k].append(len(set(rank_g[:k]) & set(rank_p[:k]))/k)\n",
        "\n",
        "        # Average precision calculation\n",
        "        K = max(ks)\n",
        "        true_set = set(rank_p[:K])\n",
        "        hits, prec_sum = 0, 0.0\n",
        "\n",
        "        for r, j in enumerate(rank_g, start=1):\n",
        "            if j in true_set:\n",
        "                hits += 1\n",
        "                prec_sum += hits / r\n",
        "                if hits == K:\n",
        "                    break\n",
        "\n",
        "        ap = prec_sum / (K if K > 0 else 1)\n",
        "        aps.append(ap)\n",
        "\n",
        "    return {\n",
        "        \"precision_at\": {k: float(np.mean(v)) for k, v in precisions.items()},\n",
        "        \"auprc\": float(np.mean(aps))\n",
        "    }\n",
        "\n",
        "# Plotting functions\n",
        "def plot_distance_scatter(Dg, Dp, title, out=\"figs/improved_combinatorial_scatter.png\"):\n",
        "    \"\"\"Plot distance correlation scatter\"\"\"\n",
        "    os.makedirs(\"figs\", exist_ok=True)\n",
        "    iu = np.triu_indices_from(Dg, k=1)\n",
        "    x, y = Dg[iu], Dp[iu]\n",
        "\n",
        "    # Sample for plotting if too many points\n",
        "    if len(x) > 20000:\n",
        "        idx = np.random.choice(len(x), 20000, replace=False)\n",
        "        x, y = x[idx], y[idx]\n",
        "\n",
        "    rho, p = spearmanr(x, y)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.scatter(x, y, s=8, alpha=0.6)\n",
        "    plt.title(f\"{title}\\nSpearman ρ={rho:.3f}, p={p:.2e}\")\n",
        "    plt.xlabel(\"Combined Embedding Distance\")\n",
        "    plt.ylabel(\"Combinatorial Perturbation Distance\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out, dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "def plot_method_comparison(results_by_method, out=\"figs/method_comparison.png\"):\n",
        "    \"\"\"Plot comparison of different combination methods\"\"\"\n",
        "    os.makedirs(\"figs\", exist_ok=True)\n",
        "\n",
        "    methods = list(results_by_method.keys())\n",
        "    rhos = [results_by_method[m]['spearman_rho'] for m in methods]\n",
        "    precisions = [results_by_method[m]['precision_at'][10] for m in methods]\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    # Correlation comparison\n",
        "    ax1.bar(methods, rhos)\n",
        "    ax1.set_ylabel('Spearman ρ')\n",
        "    ax1.set_title('Correlation by Method')\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Precision comparison\n",
        "    ax2.bar(methods, precisions)\n",
        "    ax2.set_ylabel('Precision@10')\n",
        "    ax2.set_title('Retrieval by Method')\n",
        "    ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out, dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "# Main improved analysis function\n",
        "def run_improved_combinatorial_analysis():\n",
        "    \"\"\"Run improved combinatorial analysis with gemgroup matching and multiple operators\"\"\"\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    os.makedirs(\"figs\", exist_ok=True)\n",
        "\n",
        "    print(\"=== IMPROVED COMBINATORIAL ANALYSIS ===\\n\")\n",
        "\n",
        "    # Load data\n",
        "    adata = load_gse133344_data()\n",
        "    adata = categorize_perturbations(adata)\n",
        "    generain_emb = load_generain_embeddings(files['generain'])\n",
        "\n",
        "    # IMPROVEMENT 1: Gemgroup-matched pseudobulk and effects\n",
        "    pseudobulk_df, meta_df = make_gemgroup_matched_combinatorial_pseudobulk(adata, min_cells=20)\n",
        "    effects_df = compute_gemgroup_matched_combinatorial_effects(pseudobulk_df, meta_df)\n",
        "\n",
        "    # Create target combination mapping\n",
        "    target_combinations = {}\n",
        "    for _, row in meta_df.iterrows():\n",
        "        guide = row['guide_identity']\n",
        "        targets = row['targets']\n",
        "        target_combinations[guide] = targets\n",
        "\n",
        "    # Filter to available guides in effects\n",
        "    available_guides = effects_df.index.tolist()\n",
        "    available_combinations = {g: target_combinations[g] for g in available_guides if g in target_combinations}\n",
        "\n",
        "    print(f\"Available combinations for analysis: {len(available_combinations)}\")\n",
        "\n",
        "    # IMPROVEMENT 2: Test different combination operators\n",
        "    embedding_results = test_combination_operators(available_combinations, generain_emb)\n",
        "\n",
        "    # Test each combination method\n",
        "    best_results = {}\n",
        "    best_method = None\n",
        "    best_rho = -1\n",
        "\n",
        "    for method, combined_embeddings in embedding_results.items():\n",
        "        print(f\"\\n--- Testing {method} combination method ---\")\n",
        "\n",
        "        # Align datasets\n",
        "        common_guides = list(set(effects_df.index) & set(combined_embeddings.index))\n",
        "        print(f\"Common guides for {method}: {len(common_guides)}\")\n",
        "\n",
        "        if len(common_guides) < 10:\n",
        "            print(f\"Too few guides for {method}, skipping\")\n",
        "            continue\n",
        "\n",
        "        effects_aligned = effects_df.loc[common_guides]\n",
        "        embeddings_aligned = combined_embeddings.loc[common_guides]\n",
        "\n",
        "        # Compute distances\n",
        "        Dp = pairwise_cosine(effects_aligned.values)\n",
        "        Dg = pairwise_cosine(embeddings_aligned.values)\n",
        "\n",
        "        # Statistics\n",
        "        rho, p, r_mantel, p_mantel = mantel_and_spearman(Dg, Dp, n_perms=999)\n",
        "        retrieval_results = retrieval_metrics(Dg, Dp)\n",
        "\n",
        "        method_results = {\n",
        "            'n_combinations': len(common_guides),\n",
        "            'spearman_rho': float(rho),\n",
        "            'spearman_p': float(p),\n",
        "            'mantel_r': float(r_mantel),\n",
        "            'mantel_p': float(p_mantel),\n",
        "            'precision_at': retrieval_results['precision_at'],\n",
        "            'auprc': retrieval_results['auprc']\n",
        "        }\n",
        "\n",
        "        best_results[method] = method_results\n",
        "\n",
        "        print(f\"Correlation: ρ = {rho:.3f}, p = {p:.2e}\")\n",
        "        print(f\"Mantel test: r = {r_mantel:.3f}, p = {p_mantel:.2e}\")\n",
        "        print(f\"Precision@10: {retrieval_results['precision_at'][10]:.3f}\")\n",
        "        print(f\"Random baseline: {10/(len(common_guides)-1):.3f}\")\n",
        "\n",
        "        # Track best method\n",
        "        if rho > best_rho:\n",
        "            best_rho = rho\n",
        "            best_method = method\n",
        "\n",
        "    # Save comprehensive results\n",
        "    final_results = {\n",
        "        'analysis_type': 'improved_combinatorial',\n",
        "        'improvements_applied': [\n",
        "            'gemgroup_matched_controls',\n",
        "            'multiple_combination_operators'\n",
        "        ],\n",
        "        'best_combination_method': best_method,\n",
        "        'results_by_method': best_results\n",
        "    }\n",
        "\n",
        "    with open(\"results/improved_combinatorial_metrics.json\", \"w\") as f:\n",
        "        json.dump(final_results, f, indent=2)\n",
        "\n",
        "    # Generate plots\n",
        "    if best_method and best_method in embedding_results:\n",
        "        # Plot best method\n",
        "        best_embeddings = embedding_results[best_method]\n",
        "        common_guides = list(set(effects_df.index) & set(best_embeddings.index))\n",
        "\n",
        "        effects_final = effects_df.loc[common_guides]\n",
        "        embeddings_final = best_embeddings.loc[common_guides]\n",
        "\n",
        "        Dp_final = pairwise_cosine(effects_final.values)\n",
        "        Dg_final = pairwise_cosine(embeddings_final.values)\n",
        "\n",
        "        plot_distance_scatter(Dg_final, Dp_final,\n",
        "                            f\"Improved Combinatorial Analysis ({best_method})\")\n",
        "\n",
        "    # Plot method comparison\n",
        "    if len(best_results) > 1:\n",
        "        plot_method_comparison(best_results)\n",
        "\n",
        "    # Print final results\n",
        "    print(f\"\\n=== IMPROVED COMBINATORIAL RESULTS ===\")\n",
        "    print(f\"Best combination method: {best_method}\")\n",
        "\n",
        "    if best_method and best_method in best_results:\n",
        "        best_res = best_results[best_method]\n",
        "        print(f\"Combinations analyzed: {best_res['n_combinations']}\")\n",
        "        print(f\"Correlation: ρ = {best_res['spearman_rho']:.3f}, p = {best_res['spearman_p']:.2e}\")\n",
        "        print(f\"Mantel test: r = {best_res['mantel_r']:.3f}, p = {best_res['mantel_p']:.2e}\")\n",
        "        print(f\"Precision@10: {best_res['precision_at'][10]:.3f}\")\n",
        "        print(f\"vs random: {10/(best_res['n_combinations']-1):.3f}\")\n",
        "        print(f\"AUPRC: {best_res['auprc']:.3f}\")\n",
        "\n",
        "        # Show improvement over original\n",
        "        print(f\"\\nComparison to original analysis:\")\n",
        "        print(f\"  Original: ρ = 0.081, P@10 = 0.328\")\n",
        "        print(f\"  Improved: ρ = {best_res['spearman_rho']:.3f}, P@10 = {best_res['precision_at'][10]:.3f}\")\n",
        "        print(f\"  Correlation improvement: {(best_res['spearman_rho']/0.081 - 1)*100:+.1f}%\")\n",
        "        print(f\"  Retrieval improvement: {(best_res['precision_at'][10]/0.328 - 1)*100:+.1f}%\")\n",
        "\n",
        "    return final_results\n",
        "\n",
        "# Run the improved analysis\n",
        "print(\"Ready to run improved combinatorial analysis.\")\n",
        "print(\"Execute: results = run_improved_combinatorial_analysis()\")\n",
        "results = run_improved_combinatorial_analysis()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxzi53wcUlOW",
        "outputId": "1c74e45d-502a-4f72-8c52-2f4ea4c637fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/169.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.9/169.9 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/9.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m7.3/9.7 MB\u001b[0m \u001b[31m223.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m206.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m127.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.4/204.4 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.2/58.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.4/276.4 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m129.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n",
            "Ready to run improved combinatorial analysis.\n",
            "Execute: results = run_improved_combinatorial_analysis()\n",
            "=== IMPROVED COMBINATORIAL ANALYSIS ===\n",
            "\n",
            "Loading GSE133344 dataset...\n",
            "Loaded dataset: 111445 cells x 33694 genes\n",
            "Perturbation guides: 290\n",
            "Gemgroups: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8)]\n",
            "Perturbation type distribution:\n",
            "perturbation_type\n",
            "single     57831\n",
            "dual       41759\n",
            "control    11855\n",
            "Name: count, dtype: int64\n",
            "Loading GeneRAIN embeddings from /content/drive/MyDrive/dataset-gene-embed/GeneRAIN-vec.200d.txt...\n",
            "Metadata: 31769 200\n",
            "Loaded GeneRAIN: 31769 genes, 200 dimensions\n",
            "Creating gemgroup-matched pseudobulks for combinatorial analysis...\n",
            "Created pseudobulk for 2037 guide-gemgroup combinations\n",
            "Gemgroups with data: 8\n",
            "Control combinations: 32\n",
            "Computing gemgroup-matched effects for combinatorial analysis...\n",
            "Found 4 control guides\n",
            "Computing effects for 268 target guides\n",
            "Computed effects for 268 guides\n",
            "Effect range: [-13.940, 16.340]\n",
            "Available combinations for analysis: 268\n",
            "Testing different combination operators...\n",
            "  Creating average combinations...\n",
            "Created average embeddings: 262 combinations, 200 dimensions\n",
            "  Creating sum combinations...\n",
            "Created sum embeddings: 262 combinations, 200 dimensions\n",
            "  Creating hadamard combinations...\n",
            "Created hadamard embeddings: 262 combinations, 200 dimensions\n",
            "\n",
            "--- Testing average combination method ---\n",
            "Common guides for average: 262\n",
            "Correlation: ρ = 0.080, p = 2.97e-49\n",
            "Mantel test: r = 0.080, p = 1.00e-03\n",
            "Precision@10: 0.310\n",
            "Random baseline: 0.038\n",
            "\n",
            "--- Testing sum combination method ---\n",
            "Common guides for sum: 262\n",
            "Correlation: ρ = 0.080, p = 2.97e-49\n",
            "Mantel test: r = 0.080, p = 1.00e-03\n",
            "Precision@10: 0.310\n",
            "Random baseline: 0.038\n",
            "\n",
            "--- Testing hadamard combination method ---\n",
            "Common guides for hadamard: 262\n",
            "Correlation: ρ = 0.047, p = 2.06e-18\n",
            "Mantel test: r = 0.047, p = 1.00e-03\n",
            "Precision@10: 0.133\n",
            "Random baseline: 0.038\n",
            "\n",
            "=== IMPROVED COMBINATORIAL RESULTS ===\n",
            "Best combination method: average\n",
            "Combinations analyzed: 262\n",
            "Correlation: ρ = 0.080, p = 2.97e-49\n",
            "Mantel test: r = 0.080, p = 1.00e-03\n",
            "Precision@10: 0.310\n",
            "vs random: 0.038\n",
            "AUPRC: 0.295\n",
            "\n",
            "Comparison to original analysis:\n",
            "  Original: ρ = 0.081, P@10 = 0.328\n",
            "  Improved: ρ = 0.080, P@10 = 0.310\n",
            "  Correlation improvement: -1.7%\n",
            "  Retrieval improvement: -5.5%\n"
          ]
        }
      ]
    }
  ]
}